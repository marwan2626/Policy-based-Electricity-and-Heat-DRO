"""
Out-of-sample Monte Carlo harness (v3): evaluates fixed DA policies from v2 against sampled uncertainties.

How to use (no command-line needed):
- Edit the USER CONFIG block below (EPSILON, RESULTS_CSV, SAMPLES_DIR, MAX_TRAJ, WRITE_DIAGNOSTICS, OUTDIR)
- Save file; then run it.

What it does:
- Loads a v2 results CSV (policy + baseline series) inferred from EPSILON or explicit RESULTS_CSV
- Loads consolidated samples generated by generate_samples.py (temperature, PV availability per bus, HP residual per bus)
- For each trajectory (sample_id), computes realized series (PV, HP residual) and a proxy net import
- Computes energy cost (no buy-back revenue), peak import, energy totals; aggregates per-trajectory metrics
- Optionally writes per-trajectory time series diagnostics

Notes and assumptions:
- DA plan is fixed; we simulate PV availability deviations and HP deviations (temperature-driven predictor delta + residuals)
- Proxy net import: net_import_rt = net_import_da + (hp_temp_dev + hp_residual) - (pv_realized - pv_DA_setpoint)
  This assumes other components stay at DA plan and PV deviations directly affect grid import/export.
- We do NOT apply DRCC network tightenings here; OOS is unconstrained aside from physical clipping already in samples.
- Export buy-back is assumed disabled (cost only for positive imports), consistent with v2 settings.
- Hooks are provided to extend with LDF-based flow/violation checks if sensitivities are available.
"""

from __future__ import annotations

import json
import os
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd


# ===========================
# USER CONFIG (edit these)
# ===========================
# Epsilon used in the v2 filename suffix: dso_model_v2_results_drcc_true_epsilon_{EPSILON_TOKEN}.csv
# The token uses two decimals with underscore as decimal separator, e.g., 0.05 -> "0_05"
EPSILON: float = 0.30
# When running the deterministic (no DRCC tightening) case, set RUN_DRCC_FALSE = True.
# In that mode we ignore EPSILON for locating the v2 results CSV and instead look for
# files named like: dso_model_v2_results_drcc_false*.csv
RUN_DRCC_FALSE: bool = True

# If RESULTS_CSV is None, we'll try to find a file named with the epsilon token in the current folder.
RESULTS_CSV: Optional[str] = None

# Directory containing consolidated samples from generate_samples.py
SAMPLES_DIR: str = "samples"

# Limit number of trajectories (sample_id) for a quick run. None means evaluate all.
MAX_TRAJ: Optional[int] = None

# Toggle to ignore stochastic HP residuals in OOS (Option A). When True, only temperature-driven HP deviations remain.
IGNORE_HP_RESIDUAL: bool = False

# Write per-trajectory time series diagnostics (large files). Disabled by default.
WRITE_DIAGNOSTICS: bool = False

# Output directory for v3 results
OUTDIR: str = "v3_oos"

# Threshold (in percent) above which a line or transformer loading counts as a violation step
LOADING_VIOLATION_THRESHOLD_PCT: float = 80.0

# Voltage evaluation configuration (reintroduced for full flow metrics in OOS)
VOLTAGE_EVAL_ENABLED: bool = True          # Set False to skip voltage reconstruction
VOLTAGE_SLACK_PU: float = 1.0              # Slack/reference bus voltage
VOLTAGE_MIN_PU: float = 0.90               # Base compliance band min (replicate v2 default if tightened not applied)
VOLTAGE_MAX_PU: float = 1.10               # Base compliance band max
VOLTAGE_TIGHT_MIN_PU: float | None = None  # Optionally override with tightened limits (leave None to use base)
VOLTAGE_TIGHT_MAX_PU: float | None = None
COUNT_VIOLATION_ON_ANY_BUS: bool = True    # If True, a timestep counts as 1 violation if any bus is out-of-band

# Phase 1: Use mean-centered residual for policy activation (instead of schedule-referenced residual)
USE_MEAN_CENTERED_POLICY: bool = True  # set False to use schedule-based residual directly
VALIDATE_BASELINE_ONLY: bool = False  # Stage 2b: if True (or CLI flag) run only deterministic baseline accumulation and exit

# Stage 3: Use Option A radial accumulation instead of admittance-based proxy for transformer loading in stochastic loop
USE_OPTIONA_FLOW: bool = True  # if True, compute transformer loading from accumulated downstream P/Q (consistent with v2 export)
COMPARE_FLOWS: bool = False  # if True, compute both Option A and admittance-based transformer loading for comparison

# Logging of transformer loading distributions (for CVaR / tail risk analysis)
LOG_TRAFO_LOADING: bool = True  # set True to enable logging
TRAFO_LOADING_BUFFER_SAMPLES: int = 50  # flush every N samples
TRAFO_LOADING_DIR_NAME: str = "v3_loading"  # subdirectory inside OUTDIR
TRAFO_LOADING_FILENAME_PREFIX: str = "trafo_loading_raw_epsilon_"  # suffix token + .parquet
TRAFO_LOADING_FLOAT_DTYPE = "float32"
TRAFO_LOADING_WRITE_PARQUET: bool = True  # requires pyarrow

# Full per-trajectory series export (heavy). When enabled, writes one CSV per trajectory with
# bus voltages, line & transformer loading/flows, component powers (HP, flex, base load, PV, BESS),
# residuals & balancing actions, and SoC.
EXPORT_FULL_SERIES: bool = True
FULL_SERIES_DIR: str = "v3_series"

# If only one trajectory is simulated (MAX_TRAJ == 1 or resulting n_trajectories == 1) produce
# comprehensive diagnostic plots similar to v2 (voltages, line/trafo loadings, component powers, prices, SoC).
PLOT_SINGLE_TRAJECTORY_DIAGNOSTICS: bool = True

# Use per-bus flexible and non-flexible load P/Q time series directly from v2 results CSV.
# When True: we ignore profile reconstruction and take columns:
#   load_nonflex_p_bus_{bus}_mw, load_nonflex_q_bus_{bus}_mvar,
#   load_flex_p_bus_{bus}_mw,    load_flex_q_bus_{bus}_mvar.
# If a Q column is missing for a bus we treat Q=0. Magnitudes are used exactly as stored.
LOAD_LOAD_VALUES_FROM_V2: bool = True

# Replay the v2 day-ahead BESS schedule (discharge/charge) before applying residual recourse.
# When True, we import per-bus bess_discharge_bus_{b}_mw and bess_charge_bus_{b}_mw columns (if present)
# and inject (discharge - charge) as the baseline BESS power prior to adding the affine recourse delta.
# Recourse delta is still distributed by capacity share. Missing columns default to zero.
REPLAY_V2_BESS_SCHEDULE: bool = True


# ===========================
# Helpers
# ===========================

def _epsilon_token(eps: float) -> str:
    """Format epsilon as '0_05' (two decimals, dot -> underscore)."""
    s = f"{eps:.2f}"
    return s.replace(".", "_")


def _infer_v2_csv(epsilon: float) -> Optional[str]:
    """Locate a v2 results CSV.

    Logic:
    - If RUN_DRCC_FALSE is True: ignore epsilon and search for deterministic files
      starting with 'dso_model_v2_results_drcc_false'. If multiple, pick most recent.
    - Else (DRCC / tightened case): use epsilon token to find 'drcc_true' file first;
      fallback to any file containing that token.
    """
    if RUN_DRCC_FALSE:
        prefix = "dso_model_v2_results_drcc_false"
        det_candidates = [
            f for f in os.listdir(os.getcwd())
            if f.startswith(prefix) and f.endswith('.csv')
        ]
        if not det_candidates:
            return None
        det_candidates.sort(key=lambda p: os.path.getmtime(p), reverse=True)
        return det_candidates[0]

    # DRCC (tightened) path uses epsilon token
    token = _epsilon_token(epsilon)
    canonical = f"dso_model_v2_results_drcc_true_epsilon_{token}.csv"
    if os.path.exists(canonical):
        return canonical
    # Fallback: any file with the token (keeps backward compatibility if naming shifted)
    candidates = [
        f for f in os.listdir(os.getcwd())
        if f.startswith("dso_model_v2_results_") and f.endswith('.csv') and token in f
    ]
    if candidates:
        candidates.sort(key=lambda p: os.path.getmtime(p), reverse=True)
        return candidates[0]
    return None


def _load_v2_results(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if 'timestamp' in df.columns:
        ts = pd.to_datetime(df['timestamp'])
        df.index = ts
    else:
        # Try reconstructing from meta_time_start and dt_hours
        try:
            # Use .iloc[0] instead of [0] on a list to avoid FutureWarning about positional indexing
            start_raw = df.get('meta_time_start', [None]).__getitem__(0)
            start = pd.to_datetime(start_raw) if start_raw is not None else pd.NaT
        except Exception:
            start = pd.NaT
        try:
            dt_hours = float(df.get('meta_dt_hours', [np.nan]).__getitem__(0))
        except Exception:
            dt_hours = np.nan
        if pd.isna(start) or not np.isfinite(dt_hours):
            # Leave index as RangeIndex; caller may fix from samples
            pass
        else:
            df.index = pd.date_range(start=start, periods=len(df), freq=f"{int(round(dt_hours*60))}min")
    return df


def _load_samples(samples_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    # Temperature (long)
    temp_path = os.path.join(samples_dir, 'samples_temperature_c.csv')
    pv_path = os.path.join(samples_dir, 'samples_pv.csv')
    hp_path = os.path.join(samples_dir, 'samples_hp_residual.csv')
    if not (os.path.exists(temp_path) and os.path.exists(pv_path) and os.path.exists(hp_path)):
        raise FileNotFoundError(
            f"Missing sample files in {samples_dir}. Expected samples_temperature_c.csv, samples_pv.csv, samples_hp_residual.csv"
        )

    temp_df = pd.read_csv(temp_path, parse_dates=['timestamp'])
    pv_df = pd.read_csv(pv_path, parse_dates=['timestamp'])
    hp_df = pd.read_csv(hp_path, parse_dates=['timestamp'])
    return temp_df, pv_df, hp_df


def _load_samples_meta(samples_dir: str) -> Optional[Dict[str, object]]:
    meta_path = os.path.join(samples_dir, 'samples_meta.json')
    if os.path.exists(meta_path):
        try:
            with open(meta_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return None
    return None


def _get_bus_ids_from_columns(df: pd.DataFrame, prefix: str, suffix: str) -> List[int]:
    ids: List[int] = []
    for c in df.columns:
        if c.startswith(prefix) and c.endswith(suffix):
            try:
                mid = c[len(prefix):-len(suffix)]
                ids.append(int(mid))
            except Exception:
                continue
    return sorted(list(set(ids)))


def _summarize_trajectory(
    ts: pd.DatetimeIndex,
    price_eur_mwh: np.ndarray,
    net_import_mw: np.ndarray,
    dt_hours: float,
    pv_total_mw: np.ndarray,
    hp_resid_total_mw: np.ndarray,
    temp_c: Optional[np.ndarray],
) -> Dict[str, float]:
    # Imports (positive only)
    import_mw = np.maximum(net_import_mw, 0.0)
    import_mwh = float(np.sum(import_mw * dt_hours))
    energy_cost_eur = float(np.sum(import_mw * price_eur_mwh * dt_hours))
    peak_import_mw = float(np.max(import_mw)) if len(import_mw) else 0.0
    pv_energy_mwh = float(np.sum(np.maximum(pv_total_mw, 0.0) * dt_hours))
    hp_resid_energy_mwh_abs = float(np.sum(np.abs(hp_resid_total_mw) * dt_hours))
    avg_temp_c = float(np.mean(temp_c)) if temp_c is not None and len(temp_c) else np.nan
    return {
        'import_mwh': import_mwh,
        'energy_cost_eur': energy_cost_eur,
        'peak_import_mw': peak_import_mw,
        'pv_energy_mwh': pv_energy_mwh,
        'hp_resid_energy_mwh_abs': hp_resid_energy_mwh_abs,
        'avg_temp_c': avg_temp_c,
    }


def _collect_da_series(df: pd.DataFrame, bus_ids: List[int], prefix: str, suffix: str) -> Dict[int, np.ndarray]:
    out: Dict[int, np.ndarray] = {}
    for b in bus_ids:
        col = f"{prefix}{b}{suffix}"
        if col in df.columns:
            out[b] = df[col].to_numpy(dtype=float)
    return out


def create_network_from_csv():
    network_data_path = "extracted_network_data"
    electrical_buses = pd.read_csv(os.path.join(network_data_path, "electrical_buses.csv"))
    electrical_lines = pd.read_csv(os.path.join(network_data_path, "electrical_lines.csv"))
    electrical_loads = pd.read_csv(os.path.join(network_data_path, "electrical_loads.csv"))
    electrical_pv_systems = pd.read_csv(os.path.join(network_data_path, "electrical_pv_systems.csv"))
    electrical_bess = pd.read_csv(os.path.join(network_data_path, "electrical_bess.csv"))
    electrical_transformers = pd.read_csv(os.path.join(network_data_path, "electrical_transformers.csv"))
    electrical_external_grids = pd.read_csv(os.path.join(network_data_path, "electrical_external_grids.csv"))

    class NetworkFromCSV:
        def __init__(self):
            self.sn_mva = 100.0
            self.bus = pd.DataFrame({
                'vn_kv': electrical_buses['vn_kv'],
                'name': electrical_buses['name'],
                'type': electrical_buses['type'],
                'zone': electrical_buses.get('zone', ''),
                'in_service': electrical_buses.get('in_service', True)
            })
            self.bus.index = electrical_buses['bus_id'].values
            self.line = pd.DataFrame({
                'from_bus': electrical_lines['from_bus'],
                'to_bus': electrical_lines['to_bus'],
                'length_km': electrical_lines['length_km'],
                'r_ohm_per_km': electrical_lines['r_ohm_per_km'],
                'x_ohm_per_km': electrical_lines['x_ohm_per_km'],
                'c_nf_per_km': electrical_lines.get('c_nf_per_km', 0),
                'max_i_ka': electrical_lines['max_i_ka'],
                'name': electrical_lines['name'],
                'in_service': electrical_lines.get('in_service', True)
            })
            self.trafo = pd.DataFrame({
                'hv_bus': electrical_transformers['hv_bus'],
                'lv_bus': electrical_transformers['lv_bus'],
                'sn_mva': electrical_transformers['sn_mva'],
                'vn_hv_kv': electrical_transformers['vn_hv_kv'],
                'vn_lv_kv': electrical_transformers['vn_lv_kv'],
                'vk_percent': electrical_transformers['vk_percent'],
                'vkr_percent': electrical_transformers['vkr_percent'],
                'name': electrical_transformers['name'],
                'in_service': electrical_transformers.get('in_service', True)
            })
            default_p_mw = 0.001
            default_q_mvar = 0.0
            default_controllable = False
            self.load = pd.DataFrame({
                'bus': electrical_loads['bus'],
                'p_mw': electrical_loads.get('p_mw', default_p_mw),
                'q_mvar': electrical_loads.get('q_mvar', default_q_mvar),
                'controllable': electrical_loads.get('controllable', default_controllable),
                'name': electrical_loads.get('name', 'Load'),
                'in_service': electrical_loads.get('in_service', True)
            })
            self.sgen = pd.DataFrame({
                'bus': electrical_pv_systems['bus_id'],
                'p_mw': electrical_pv_systems['capacity_kw'] / 1000.0,
                'q_mvar': electrical_pv_systems.get('q_mvar', 0.0),
                'name': electrical_pv_systems.get('name', 'PV'),
                'in_service': electrical_pv_systems.get('in_service', True),
                'type': electrical_pv_systems.get('type', 'PV')
            })
            # Add BESS as sgen entries
            bess_sgen = pd.DataFrame()
            if electrical_bess is not None and len(electrical_bess) > 0:
                if 'max_p_mw' in electrical_bess.columns:
                    p_mw_series = electrical_bess['max_p_mw'].astype(float)
                elif 'p_mw' in electrical_bess.columns:
                    p_mw_series = electrical_bess['p_mw'].astype(float)
                elif 'max_p_kw' in electrical_bess.columns:
                    p_mw_series = electrical_bess['max_p_kw'].astype(float) / 1000.0
                elif 'p_kw' in electrical_bess.columns:
                    p_mw_series = electrical_bess['p_kw'].astype(float) / 1000.0
                else:
                    p_mw_series = pd.Series(0.0, index=electrical_bess.index)
                bess_sgen = pd.DataFrame({
                    'bus': electrical_bess['bus'],
                    'p_mw': p_mw_series,
                    'q_mvar': electrical_bess.get('q_mvar', 0.0),
                    'name': electrical_bess.get('name', 'BESS'),
                    'in_service': electrical_bess.get('in_service', True),
                    'type': electrical_bess.get('type', 'BESS')
                })
                try:
                    bess_sgen['bus'] = bess_sgen['bus'].astype(int)
                except Exception:
                    pass
            if bess_sgen is not None and len(bess_sgen) > 0:
                for col in ['bus', 'p_mw', 'q_mvar', 'name', 'in_service', 'type']:
                    if col not in self.sgen.columns:
                        self.sgen[col] = None
                    if col not in bess_sgen.columns:
                        bess_sgen[col] = None
                self.sgen = pd.concat([self.sgen, bess_sgen], ignore_index=True, sort=False).reset_index(drop=True)
            self.ext_grid = pd.DataFrame({
                'bus': electrical_external_grids['bus'],
                'vm_pu': electrical_external_grids.get('vm_pu', 1.0),
                'va_degree': electrical_external_grids.get('va_degree', 0.0),
                'name': electrical_external_grids.get('name', 'External Grid'),
                'in_service': electrical_external_grids.get('in_service', True)
            })
            self.controller = pd.DataFrame(columns=['object'])
    return NetworkFromCSV()


def calculate_gbus_matrix(net):
    num_buses = len(net.bus)
    Gbus = np.zeros((num_buses, num_buses))
    for line in net.line.itertuples():
        from_bus = line.from_bus; to_bus = line.to_bus
        base_voltage = net.bus.at[from_bus, 'vn_kv']
        Z_base = base_voltage ** 2 / net.sn_mva
        x_pu = line.x_ohm_per_km * line.length_km / Z_base
        r_pu = line.r_ohm_per_km * line.length_km / Z_base
        Y_series = 1 / (r_pu + 1j * x_pu)
        g = np.real(Y_series)
        Gbus[from_bus, from_bus] += g
        Gbus[to_bus, to_bus] += g
        Gbus[from_bus, to_bus] -= g
        Gbus[to_bus, from_bus] -= g
    return Gbus


def calculate_bbus_matrix(net):
    num_buses = len(net.bus)
    Bbus = np.zeros((num_buses, num_buses))
    for line in net.line.itertuples():
        from_bus = line.from_bus; to_bus = line.to_bus
        base_voltage = net.bus.at[from_bus, 'vn_kv']
        Z_base = base_voltage ** 2 / net.sn_mva
        x_pu = line.x_ohm_per_km * line.length_km / Z_base
        r_pu = line.r_ohm_per_km * line.length_km / Z_base
        Y_series = 1 / (r_pu + 1j * x_pu)
        b = np.imag(Y_series)
        # Use the imaginary part directly (note: typically negative for inductive lines)
        Bbus[from_bus, from_bus] += b
        Bbus[to_bus, to_bus] += b
        Bbus[from_bus, to_bus] -= b
        Bbus[to_bus, from_bus] -= b
    return Bbus


def build_Ybus(net):
    num_buses = len(net.bus)
    Ybus = np.zeros((num_buses, num_buses), dtype=complex)
    # Lines
    for line in net.line.itertuples():
        fb = int(line.from_bus); tb = int(line.to_bus)
        base_voltage = float(net.bus.at[fb, 'vn_kv'])
        Z_base = base_voltage ** 2 / float(net.sn_mva)
        r_pu = float(line.r_ohm_per_km) * float(line.length_km) / Z_base
        x_pu = float(line.x_ohm_per_km) * float(line.length_km) / Z_base
        z = complex(r_pu, x_pu)
        if abs(z) < 1e-12:
            continue
        y = 1.0 / z
        Ybus[fb, fb] += y
        Ybus[tb, tb] += y
        Ybus[fb, tb] -= y
        Ybus[tb, fb] -= y
    # Transformers (approximate series model on common Sbase)
    if hasattr(net, 'trafo') and len(net.trafo) > 0:
        Sbase_sys = float(net.sn_mva)
        for tr in net.trafo.itertuples():
            hv = int(tr.hv_bus); lv = int(tr.lv_bus)
            sn_tr = float(tr.sn_mva) if hasattr(tr, 'sn_mva') else Sbase_sys
            vk_pu = float(tr.vk_percent) / 100.0 if hasattr(tr, 'vk_percent') else 0.05
            r_pu = float(tr.vkr_percent) / 100.0 if hasattr(tr, 'vkr_percent') else 0.01
            x_sq = max(vk_pu**2 - r_pu**2, 1e-8)
            x_pu = float(np.sqrt(x_sq))
            z_tr_pu_own = complex(r_pu, x_pu)
            # Convert to system Sbase
            z_tr_pu_sys = z_tr_pu_own * (Sbase_sys / max(sn_tr, 1e-6))
            if abs(z_tr_pu_sys) < 1e-12:
                continue
            y_tr = 1.0 / z_tr_pu_sys
            Ybus[hv, hv] += y_tr
            Ybus[lv, lv] += y_tr
            Ybus[hv, lv] -= y_tr
            Ybus[lv, hv] -= y_tr
    return Ybus


def build_electrical_time_series(time_index: pd.DatetimeIndex) -> Dict[str, np.ndarray]:
    profiles_path = 'vdi_profiles/all_house_profiles.csv'
    if not os.path.exists(profiles_path):
        return {}
    profiles_df = pd.read_csv(profiles_path, index_col=0)
    profiles_df.index = pd.to_datetime(profiles_df.index)
    window_df = profiles_df.reindex(time_index).fillna(0.0)

    # Load electrical load definitions
    electrical_loads = pd.read_csv(os.path.join('extracted_network_data', 'electrical_loads.csv'))
    load_names = electrical_loads['name'].astype(str).fillna('')
    def _norm(s: str) -> str:
        return s.replace('.', '_').replace('-', '_').lower()

    cols_norm = {_norm(c): c for c in window_df.columns}
    series_map: Dict[str, np.ndarray] = {}
    for el_orig in load_names:
        if not el_orig:
            continue
        el_norm = _norm(el_orig)
        candidates = [f"{el_norm}_electricity", f"{el_norm}.electricity"]
        chosen = None
        for cand in candidates:
            if cand in cols_norm:
                chosen = cols_norm[cand]; break
        if chosen is None:
            # try swapped separators
            for base in (el_norm.replace('.', '_'), el_norm.replace('_', '.')):
                for suff in ('_electricity', '.electricity'):
                    key = _norm(base + suff)
                    if key in cols_norm:
                        chosen = cols_norm[key]; break
                if chosen: break
        if chosen is None:
            series_map[el_orig] = np.zeros(len(time_index))
        else:
            series_map[el_orig] = window_df[chosen].to_numpy()
    return series_map


def apply_recourse_for_step(residual: float, v2_row: pd.Series, pv_sched_mw: float) -> Tuple[float, float]:
    """Policy commands (open-loop) given the current residual before local actions.

    residual > 0  => deficit (need supply / discharge)
    residual < 0  => surplus (need absorption / curtail / charge)

    Returns (p_bess_cmd_mw, extra_curt_cmd_mw)

    Notes:
    - We no longer generate u_plus / u_minus here; PCC imbalance is the *post-action* residual.
    - Extra curtail only acts when residual < 0 (surplus) and is capped by scheduled PV injection (pv_sched_mw).
    - rho coefficients are not applied in OOS; imbalance settlement is the terminal residual.
    """
    d_plus = max(0.0, float(residual))
    d_minus = max(0.0, -float(residual))

    lam0 = float(v2_row.get('lambda0_mw', 0.0))
    lam_p = float(v2_row.get('lambda_plus', 0.0))
    lam_m = float(v2_row.get('lambda_minus', 0.0))
    chi0 = float(v2_row.get('chi0_mw', 0.0))
    chi_m = float(v2_row.get('chi_minus', 0.0))

    # Unclipped BESS command (+ discharge, - charge)
    p_bess_cmd = lam0 + lam_p * d_plus - lam_m * d_minus

    extra_curt = 0.0
    if d_minus > 0.0 and pv_sched_mw > 0.0:
        # Curtail scheduled PV (cannot exceed scheduled injection)
        extra_curt = max(0.0, min(chi0 + chi_m * d_minus, pv_sched_mw))
    return p_bess_cmd, extra_curt


def main() -> None:
    os.makedirs(OUTDIR, exist_ok=True)

    v2_csv = RESULTS_CSV or _infer_v2_csv(EPSILON)
    if not v2_csv or not os.path.exists(v2_csv):
        raise FileNotFoundError(
            "Could not resolve v2 results CSV. Set RESULTS_CSV or check EPSILON token matches filename."
        )

    # Load v2 and samples
    df = _load_v2_results(v2_csv)
    temp_df, pv_df, hp_df = _load_samples(SAMPLES_DIR)
    samples_meta = _load_samples_meta(SAMPLES_DIR) or {}

    # Canonical sample index
    temp_ts = pd.to_datetime(temp_df['timestamp'])
    pv_ts = pd.to_datetime(pv_df['timestamp'])
    hp_ts = pd.to_datetime(hp_df['timestamp'])
    sample_index = pd.DatetimeIndex(sorted(set(temp_ts) & set(pv_ts) & set(hp_ts)))

    # --- Runtime overrides for mean-centering flag ---
    # Priority order: command-line arg > environment variable > user config above.
    # CLI usage examples (optional):
    #   python dso_model_v3.py --mean-centered
    #   python dso_model_v3.py --schedule-residual
    import sys as _sys
    global USE_MEAN_CENTERED_POLICY, VALIDATE_BASELINE_ONLY, MAX_TRAJ, COMPARE_FLOWS  # allow override of module-level settings
    argv = list(_sys.argv[1:])
    # Residual basis overrides
    if any(arg in ('--mean-centered', '--mean_centered') for arg in argv):
        USE_MEAN_CENTERED_POLICY = True
    elif any(arg in ('--schedule-residual', '--schedule_residual', '--no-mean-centered') for arg in argv):
        USE_MEAN_CENTERED_POLICY = False
    # Baseline validation flag
    if any(arg in ('--validate-baseline', '--baseline-validate') for arg in argv):
        VALIDATE_BASELINE_ONLY = True
    if any(arg in ('--no-baseline-validate','--no-baseline','--skip-baseline-validate') for arg in argv):
        VALIDATE_BASELINE_ONLY = False
    # Flow comparison flag
    if any(arg in ('--compare-flows', '--flow-compare') for arg in argv):
        COMPARE_FLOWS = True
    # Max trajectories CLI
    if '--max-traj' in argv:
        try:
            idx = argv.index('--max-traj')
            MAX_TRAJ = int(argv[idx+1])
            print(f"[config] Limiting trajectories to first {MAX_TRAJ} via --max-traj")
        except Exception:
            print('[WARN] --max-traj provided but could not parse integer; ignoring.')
    # Env var for residual basis if not explicitly set
    env_flag = os.getenv('V3_USE_MEAN_CENTERED')  # '1','true','yes' => True; '0','false','no' => False
    if env_flag is not None:
        val = env_flag.strip().lower()
        if val in ('1','true','t','yes','y'):
            USE_MEAN_CENTERED_POLICY = True
        elif val in ('0','false','f','no','n'):
            USE_MEAN_CENTERED_POLICY = False

    print(f"[config] USE_MEAN_CENTERED_POLICY = {USE_MEAN_CENTERED_POLICY} | residual basis = {'mean_centered' if USE_MEAN_CENTERED_POLICY else 'schedule'}")
    if VALIDATE_BASELINE_ONLY:
        print("[config] VALIDATE_BASELINE_ONLY = True (will compute baseline transformer loading using Option A accumulation and exit)")
    if COMPARE_FLOWS:
        print("[config] COMPARE_FLOWS = True (dual computation of Option A & admittance transformer loadings)")

    # Fix v2 timeline if needed
    index = df.index
    index_is_datetime = isinstance(index, pd.DatetimeIndex)
    needs_timeline_fix = (not index_is_datetime) or (index_is_datetime and len(index) > 0 and getattr(index[0], 'year', 1970) < 2000)
    if needs_timeline_fix:
        if len(sample_index) >= len(df):
            index = sample_index[:len(df)]
        else:
            min_len = min(len(sample_index), len(df))
            index = sample_index[:min_len]
            df = df.iloc[:min_len].copy()
        df.index = index

    # dt_hours
    if 'meta_dt_hours' in df.columns:
        try:
            dt_hours = float(df['meta_dt_hours'].iloc[0])
        except Exception:
            dt_hours = float(samples_meta.get('dt_hours', 1.0))
    else:
        dt_hours = float(samples_meta.get('dt_hours', 0.0))
        if not np.isfinite(dt_hours) or dt_hours <= 0:
            try:
                dt_hours = float(pd.Series(index).diff().dropna().median() / pd.Timedelta(hours=1))
            except Exception:
                dt_hours = 1.0
        if not np.isfinite(dt_hours) or dt_hours <= 0:
            dt_hours = 1.0

    price = (
        df['electricity_price_eur_mwh'].to_numpy(dtype=float)
        if 'electricity_price_eur_mwh' in df.columns
        else np.zeros(len(index))
    )

    # Load HP predictor coefficients from v2 export meta (fallback to v2 defaults)
    def _get_meta_float(col: str, default: float) -> float:
        val = df.get(col, None)
        if val is None:
            return default
        try:
            if isinstance(val, (list, tuple)):
                return float(val[0])
            if isinstance(val, pd.Series):
                return float(val.iloc[0])
            return float(val)
        except Exception:
            return default
    HP_PRED_PMAX = _get_meta_float('meta_hp_pred_pmax_mw', 0.30)
    HP_COEFF_B0 = _get_meta_float('meta_hp_coeff_b0', 0.331877)
    HP_COEFF_BHDD = _get_meta_float('meta_hp_coeff_bhdd', 0.015908)
    HP_COEFF_BPI = _get_meta_float('meta_hp_coeff_bpi', -0.000492)
    HP_COEFF_BTAV = _get_meta_float('meta_hp_coeff_btav', -0.014595)
    HP_COEFF_A1 = _get_meta_float('meta_hp_coeff_a1', 0.013139)
    HP_COEFF_A2 = _get_meta_float('meta_hp_coeff_a2', -0.006540)
    HP_PRED_TBASE_C = 10.0

    # Baseline map for HP predictor (per timestamp)
    try:
        base_df = pd.read_csv('hp_baseline_profile.csv')
        _base_map = {(int(r.weekday), int(r.hour), int(r.minute)): float(r.P_base_MW) for r in base_df.itertuples(index=False)}
        _base_mean = float(base_df['P_base_MW'].mean())
    except Exception:
        _base_map = {}
        _base_mean = 0.0
    def baseline_lookup(dt: pd.Timestamp) -> float:
        return _base_map.get((dt.weekday(), dt.hour, dt.minute), _base_mean)

    # Baselines
    # Day-ahead external net import (import - export) for settlement baseline
    if 'ext_grid_import_mw' in df.columns and 'ext_grid_export_mw' in df.columns:
        base_ext_net = (df['ext_grid_import_mw'].to_numpy(dtype=float) - df['ext_grid_export_mw'].to_numpy(dtype=float))
    else:
        # Fallback to net_grid_power_mw (may already embed internal balances)
        base_ext_net = df['net_grid_power_mw'].to_numpy(dtype=float) if 'net_grid_power_mw' in df.columns else np.zeros(len(index))
    base_net_import = base_ext_net  # keep legacy name for downstream references
    pv_bus_ids = _get_bus_ids_from_columns(df, 'pv_avail_bus_', '_mw')
    pv_gen_bus_ids = _get_bus_ids_from_columns(df, 'pv_gen_bus_', '_mw')
    # Initial HP bus ids inferred from v2 result columns (may include artifacts if naming pattern over-matches)
    hp_bus_ids_raw = _get_bus_ids_from_columns(df, 'hp_elec_bus_', '_mw')
    base_pv_by_bus = _collect_da_series(df, pv_bus_ids, 'pv_avail_bus_', '_mw')
    base_pv_da_by_bus = _collect_da_series(df, pv_gen_bus_ids, 'pv_gen_bus_', '_mw')
    base_hp_by_bus_raw = _collect_da_series(df, hp_bus_ids_raw, 'hp_elec_bus_', '_mw')
    # Flexible load exports (baseline, realized, curtailment) added in v2 refactor
    flex_baseline_bus_ids = _get_bus_ids_from_columns(df, 'baseline_flex_bus_', '_mw')
    flex_realized_bus_ids = _get_bus_ids_from_columns(df, 'flex_load_bus_', '_mw')
    flex_curt_bus_ids = _get_bus_ids_from_columns(df, 'curt_bus_', '_mw')
    flex_baseline_by_bus = _collect_da_series(df, flex_baseline_bus_ids, 'baseline_flex_bus_', '_mw') if flex_baseline_bus_ids else {}
    flex_realized_by_bus = _collect_da_series(df, flex_realized_bus_ids, 'flex_load_bus_', '_mw') if flex_realized_bus_ids else {}
    flex_curt_by_bus = _collect_da_series(df, flex_curt_bus_ids, 'curt_bus_', '_mw') if flex_curt_bus_ids else {}
    # Aggregate flexible load series (they are deterministic across OOS trajectories)
    if flex_baseline_by_bus:
        flex_baseline_total = np.sum(list(flex_baseline_by_bus.values()), axis=0)
    else:
        flex_baseline_total = np.zeros(len(index))
    if flex_realized_by_bus:
        flex_realized_total = np.sum(list(flex_realized_by_bus.values()), axis=0)
    else:
        flex_realized_total = np.zeros(len(index))
    if flex_curt_by_bus:
        flex_curt_total = np.sum(list(flex_curt_by_bus.values()), axis=0)
    else:
        # Fallback: difference baseline - realized if curtail columns absent
        flex_curt_total = np.maximum(0.0, flex_baseline_total - flex_realized_total)
    # ycap_mw (aggregate curtailment) consistency check
    if 'ycap_mw' in df.columns:
        try:
            ycap_series = df['ycap_mw'].to_numpy(dtype=float)
            # Only warn if discrepancy is material
            diff_norm = np.max(np.abs(ycap_series - flex_curt_total)) if len(ycap_series) else 0.0
            if diff_norm > 1e-6:
                print(f"[flex-load] Warning: ycap_mw mismatch vs summed curt_bus_ series (max abs diff {diff_norm:.3e})")
        except Exception:
            ycap_series = None
    else:
        ycap_series = None
    # Pre-compute flexible load energy / power metrics (constant across trajectories)
    if dt_hours > 0 and len(flex_baseline_total):
        flex_metrics_static = {
            'flex_baseline_energy_mwh': float(np.sum(flex_baseline_total * dt_hours)),
            'flex_realized_energy_mwh': float(np.sum(flex_realized_total * dt_hours)),
            'flex_curtail_energy_mwh': float(np.sum(flex_curt_total * dt_hours)),
            'flex_avg_curtailment_mw': float(np.mean(flex_curt_total)),
            'flex_peak_curtailment_mw': float(np.max(flex_curt_total)) if len(flex_curt_total) else 0.0,
        }
        baseline_energy = flex_metrics_static['flex_baseline_energy_mwh']
        if baseline_energy > 0:
            flex_metrics_static['flex_energy_curtailed_pct'] = 100.0 * flex_metrics_static['flex_curtail_energy_mwh'] / baseline_energy
        else:
            flex_metrics_static['flex_energy_curtailed_pct'] = np.nan
        if ycap_series is not None and len(ycap_series) == len(flex_curt_total):
            flex_metrics_static['flex_ycap_energy_mwh'] = float(np.sum(ycap_series * dt_hours))
    else:
        flex_metrics_static = {
            'flex_baseline_energy_mwh': np.nan,
            'flex_realized_energy_mwh': np.nan,
            'flex_curtail_energy_mwh': np.nan,
            'flex_avg_curtailment_mw': np.nan,
            'flex_peak_curtailment_mw': np.nan,
            'flex_energy_curtailed_pct': np.nan,
        }
    # Log detection summary
    if flex_baseline_by_bus:
        print(f"[flex-load] Detected {len(flex_baseline_by_bus)} flexible load buses (baseline). Total baseline energy = {flex_metrics_static['flex_baseline_energy_mwh']:.6f} MWh")
        print(f"[flex-load] Realized energy = {flex_metrics_static['flex_realized_energy_mwh']:.6f} MWh | Curtailment energy = {flex_metrics_static['flex_curtail_energy_mwh']:.6f} MWh ({flex_metrics_static['flex_energy_curtailed_pct']:.3f}%)")
    else:
        print("[flex-load] No flexible load columns detected in v2 results CSV.")
    # DA PV setpoints (what was actually scheduled in v2)
    base_pv_da_total = np.sum(list(base_pv_da_by_bus.values()), axis=0) if base_pv_da_by_bus else np.zeros(len(index))
    # Forecast (mean) PV availability total for mean-centering policy residuals
    pv_mean_total = np.sum(list(base_pv_by_bus.values()), axis=0) if base_pv_by_bus else np.zeros(len(index))

    # Filter samples to our timestamps
    temp_df = temp_df[temp_df['timestamp'].isin(index)]
    pv_df = pv_df[pv_df['timestamp'].isin(index)]
    hp_df = hp_df[hp_df['timestamp'].isin(index)]

    # Sample IDs
    sample_ids = sorted(set(pv_df['sample_id'].unique()) & set(hp_df['sample_id'].unique()) & set(temp_df['sample_id'].unique()))
    if MAX_TRAJ is not None:
        sample_ids = sample_ids[: int(MAX_TRAJ)]
    # Optional environment override for quick testing (does not alter stored config)
    env_max = os.getenv('V3_MAX_TRAJ')
    if env_max:
        try:
            env_lim = int(env_max)
            if env_lim > 0:
                sample_ids = sample_ids[:env_lim]
                print(f"[v3] ENV override: limiting trajectories to {env_lim} via V3_MAX_TRAJ")
        except Exception:
            pass
    if not sample_ids:
        raise RuntimeError("No overlapping sample_ids found across PV/HP/Temperature samples")

    # Build network and matrices once
    net = create_network_from_csv()

    # --- Stage 2 Option A network reconstruction data ---
    # Parse downstream map from v2 export if present; otherwise rebuild later.
    downstream_map_json = df.get('meta_downstream_map_json')
    downstream_map: Dict[int, List[int]] = {}
    if downstream_map_json is not None:
        try:
            import json as _json
            raw = downstream_map_json.iloc[0] if hasattr(downstream_map_json, 'iloc') else None
            if isinstance(raw, str) and raw.strip():
                parsed = _json.loads(raw)
                downstream_map = {int(k): [int(c) for c in v] for k, v in parsed.items()}
        except Exception:
            downstream_map = {}
    # Build parent map (single parent per bus for radial assumption)
    parent_map: Dict[int, int] = {}
    if downstream_map:
        for p, children in downstream_map.items():
            for c in children:
                parent_map[c] = p

    # Extract baseline net P/Q injections per bus if available
    baseline_net_p: Dict[int, np.ndarray] = {}
    baseline_net_q: Dict[int, np.ndarray] = {}
    bus_indices = list(net.bus.index)
    for bus in bus_indices:
        colP = f'baseline_net_p_bus_{bus}_mw'
        colQ = f'baseline_net_q_bus_{bus}_mvar'
        if colP in df.columns:
            baseline_net_p[bus] = df[colP].to_numpy(dtype=float)
        if colQ in df.columns:
            baseline_net_q[bus] = df[colQ].to_numpy(dtype=float)

    # Fallback: if downstream_map missing we will reconstruct later when Option A engine is finalized.
    def _build_downstream_from_lines() -> Dict[int, List[int]]:
        dm: Dict[int, List[int]] = {int(b): [] for b in net.bus.index}
        try:
            # Assume radial: each line defines from_bus -> to_bus direction consistent with CSV; adopt that as parent->child
            for line in net.line.itertuples():
                fb = int(line.from_bus); tb = int(line.to_bus)
                dm[fb].append(tb)
            # Include transformers (hv -> lv)
            for tr in net.trafo.itertuples():
                hv = int(tr.hv_bus); lv = int(tr.lv_bus)
                dm[hv].append(lv)
        except Exception:
            pass
        return dm
    if not downstream_map:
        downstream_map = _build_downstream_from_lines()
        parent_map = {c: p for p, ch in downstream_map.items() for c in ch}

    # Prepare post-order (children first) traversal order for accumulation (leaves have no children)
    post_order = sorted(downstream_map.keys(), key=lambda b: len(downstream_map[b]))

    def accumulate_flows(P_inj: np.ndarray, Q_inj: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Accumulate downstream injections to obtain per-bus apparent power parents see.

        P_inj/Q_inj: arrays indexed by bus index order net.bus.index
        Returns (P_accum, Q_accum) arrays same shape.
        """
        P_acc = P_inj.copy(); Q_acc = Q_inj.copy()
        # Walk buses in order of increasing subtree size (leaves first), propagate up one level to parent
        for b in post_order:
            if b in parent_map:
                p = parent_map[b]
                P_acc[p] += P_acc[b]
                Q_acc[p] += Q_acc[b]
        return P_acc, Q_acc

    # --- Stage 2b: Deterministic baseline validation path ---
    if VALIDATE_BASELINE_ONLY:
        if not baseline_net_p:
            print("[baseline-validation] No baseline_net_p_* columns found; re-run v2 export after Stage 1 changes.")
            return
        horizon = len(next(iter(baseline_net_p.values())))
        n_buses = len(bus_indices)
        # Build matrices (time, bus)
        P_base_mat = np.zeros((horizon, n_buses), dtype=float)
        Q_base_mat = np.zeros((horizon, n_buses), dtype=float)
        bus_pos = {b: i for i, b in enumerate(bus_indices)}
        for b, arr in baseline_net_p.items():
            P_base_mat[:, bus_pos[b]] = arr
        for b, arr in baseline_net_q.items():
            Q_base_mat[:, bus_pos[b]] = arr
        # Accumulate each timestep
        trafo_loading_series: Dict[int, np.ndarray] = {idx: np.zeros(horizon, dtype=float) for idx in net.trafo.index}
        for t in range(horizon):
            P_acc, Q_acc = accumulate_flows(P_base_mat[t], Q_base_mat[t])
            for tr in net.trafo.itertuples():
                lv = int(tr.lv_bus)
                idx_tr = tr.Index
                S_lv = np.sqrt(P_acc[lv]**2 + Q_acc[lv]**2)
                loading_pct = (S_lv / float(tr.sn_mva)) * 100.0 if tr.sn_mva > 0 else 0.0
                trafo_loading_series[idx_tr][t] = loading_pct
        # Summary stats
        for idx_tr, series in trafo_loading_series.items():
            print(f"[baseline-validation] Trafo {idx_tr}: max={series.max():.2f}% avg={series.mean():.2f}% >80% steps={(series>80).sum()} / {horizon}")
        # Write CSV
        import pandas as _pd
        out_df = _pd.DataFrame({'t': list(range(horizon))})
        for idx_tr, series in trafo_loading_series.items():
            out_df[f'trafo_{idx_tr}_loading_pct'] = series
        token = _epsilon_token(EPSILON)
        out_path = os.path.join(OUTDIR, f'baseline_optionA_transformer_loading_{token}.csv')
        try:
            out_df.to_csv(out_path, index=False)
            print(f"[baseline-validation] Wrote {out_path}")
        except Exception as e:
            print(f"[baseline-validation] Failed to write CSV: {e}")
        return

    # Radial accumulation for flows; now also (optionally) reconstruct approximate voltage profile along radial tree.
    slack_bus_index = int(net.ext_grid.bus.iloc[0]) if hasattr(net, 'ext_grid') and len(net.ext_grid) else 0
    non_slack_buses = [b for b in net.bus.index if b != slack_bus_index]

    # Re-identify HP load buses directly from the network (names starting with 'HP'),
    # ensuring "per-device" semantics (Option A). This guards against accidental
    # inflation from column pattern matching.
    try:
        hp_load_buses_from_net = sorted(set(
            net.load.loc[
                net.load['name'].astype(str).str.startswith('HP', na=False), 'bus'
            ].astype(int).tolist()
        ))
    except Exception:
        hp_load_buses_from_net = []

    # Intersect with raw v2 column-derived list to avoid including buses that have
    # no DA schedule (conservative) but fall back to network list if intersection empty.
    hp_bus_ids_final = sorted(set(hp_load_buses_from_net) & set(base_hp_by_bus_raw.keys()))
    if not hp_bus_ids_final:
        hp_bus_ids_final = hp_load_buses_from_net if hp_load_buses_from_net else list(base_hp_by_bus_raw.keys())

    # Rebuild base_hp_by_bus using the final set
    base_hp_by_bus = {b: base_hp_by_bus_raw[b] for b in hp_bus_ids_final if b in base_hp_by_bus_raw}

    # Diagnostic: compare per-device average DA schedule vs per-device predictor capacity
    if base_hp_by_bus:
        da_hp_total_series = np.sum(list(base_hp_by_bus.values()), axis=0)
        da_hp_mean_total = float(np.mean(da_hp_total_series)) if len(da_hp_total_series) else 0.0
        n_hp_devices = len(base_hp_by_bus)
        da_hp_mean_per_device = da_hp_mean_total / max(1, n_hp_devices)
        print(f"[HP-Scaling] Detected {n_hp_devices} HP devices from network (Option A).")
        print(f"[HP-Scaling] DA total HP mean = {da_hp_mean_total:.6f} MW | per-device mean = {da_hp_mean_per_device:.6f} MW")
        print(f"[HP-Scaling] HP_PRED_PMAX (per device) = {HP_PRED_PMAX:.6f} MW")
    else:
        print("[HP-Scaling] No HP devices detected; predictor deviation will be zero.")

    # Load mapping for non-flex loads from VDI
    time_index = index
    # Build electrical time series only if not sourcing directly from v2
    elec_ts_map = {} if LOAD_LOAD_VALUES_FROM_V2 else build_electrical_time_series(time_index)

    # Optionally extract per-bus flexible / non-flex load P/Q directly from v2 results
    nonflex_p_by_bus: Dict[int, np.ndarray] = {}
    nonflex_q_by_bus: Dict[int, np.ndarray] = {}
    flex_p_by_bus: Dict[int, np.ndarray] = {}
    flex_q_by_bus: Dict[int, np.ndarray] = {}
    if LOAD_LOAD_VALUES_FROM_V2:
        for col in df.columns:
            if col.startswith('load_nonflex_p_bus_') and col.endswith('_mw'):
                try:
                    bus = int(col[len('load_nonflex_p_bus_'):-len('_mw')])
                except Exception:
                    continue
                nonflex_p_by_bus[bus] = df[col].to_numpy(dtype=float)
            elif col.startswith('load_nonflex_q_bus_') and col.endswith('_mvar'):
                try:
                    bus = int(col[len('load_nonflex_q_bus_'):-len('_mvar')])
                except Exception:
                    continue
                nonflex_q_by_bus[bus] = df[col].to_numpy(dtype=float)
            elif col.startswith('load_flex_p_bus_') and col.endswith('_mw'):
                try:
                    bus = int(col[len('load_flex_p_bus_'):-len('_mw')])
                except Exception:
                    continue
                flex_p_by_bus[bus] = df[col].to_numpy(dtype=float)
            elif col.startswith('load_flex_q_bus_') and col.endswith('_mvar'):
                try:
                    bus = int(col[len('load_flex_q_bus_'):-len('_mvar')])
                except Exception:
                    continue
                flex_q_by_bus[bus] = df[col].to_numpy(dtype=float)

        # Zero-fill missing Q where P exists
        for b in list(nonflex_p_by_bus.keys()):
            if b not in nonflex_q_by_bus:
                nonflex_q_by_bus[b] = np.zeros_like(nonflex_p_by_bus[b])
        for b in list(flex_p_by_bus.keys()):
            if b not in flex_q_by_bus:
                flex_q_by_bus[b] = np.zeros_like(flex_p_by_bus[b])

        print(f"[v3] Loaded P/Q loads from v2: nonflex buses={len(nonflex_p_by_bus)}, flex buses={len(flex_p_by_bus)}")
        # Basic length consistency check
        horizon = len(time_index)
        for dmap_name, dmap in [('nonflex_p', nonflex_p_by_bus), ('flex_p', flex_p_by_bus)]:
            for b, arr in dmap.items():
                if len(arr) != horizon:
                    raise ValueError(f"Length mismatch for {dmap_name} bus {b}: expected {horizon}, got {len(arr)}. Ensure v2 CSV horizon matches samples.")

    # Helpers for sgen caps
    if 'type' in net.sgen.columns:
        pv_mask = net.sgen['type'].astype(str).str.contains('PV', na=False)
        bess_mask = net.sgen['type'].astype(str).str.contains('BESS', na=False)
    else:
        pv_mask = pd.Series([True] * len(net.sgen), index=net.sgen.index)
        bess_mask = pd.Series([False] * len(net.sgen), index=net.sgen.index)
    pv_caps_by_bus = net.sgen.loc[pv_mask].groupby('bus')['p_mw'].sum().to_dict()
    bess_caps_by_bus = net.sgen.loc[bess_mask].groupby('bus')['p_mw'].sum().to_dict()
    bess_pmax_total = float(sum(bess_caps_by_bus.values())) if bess_caps_by_bus else 0.0

    # Optional: import baseline BESS schedule (per-bus discharge/charge) from v2
    baseline_bess_p_by_bus: Dict[int, np.ndarray] = {}
    if REPLAY_V2_BESS_SCHEDULE and bess_caps_by_bus:
        horizon = len(time_index)
        for col in df.columns:
            if col.startswith('bess_discharge_bus_') and col.endswith('_mw'):
                try:
                    bus = int(col[len('bess_discharge_bus_'):-len('_mw')])
                except Exception:
                    continue
                dis = df[col].to_numpy(dtype=float)
                ch_col = f"bess_charge_bus_{bus}_mw"
                ch = df[ch_col].to_numpy(dtype=float) if ch_col in df.columns else np.zeros_like(dis)
                # Baseline net BESS power: discharge positive, charge negative
                net_series = dis - ch
                if len(net_series) != horizon:
                    continue
                baseline_bess_p_by_bus[bus] = net_series
        if baseline_bess_p_by_bus:
            print(f"[v3] Imported baseline BESS schedule for {len(baseline_bess_p_by_bus)} bus(es).")
        else:
            print("[v3] Baseline BESS schedule replay enabled but no per-bus discharge/charge columns found; defaulting to zero baseline.")

    # Aggregate baseline BESS net power (sum across buses) per timestep for SoC evolution convenience
    if baseline_bess_p_by_bus:
        try:
            baseline_bess_total_series = np.sum(list(baseline_bess_p_by_bus.values()), axis=0)
        except Exception:
            baseline_bess_total_series = None
    else:
        baseline_bess_total_series = None

    # Optional: initialize SoC from v2 exported energy columns if present (sum per-bus energy at t=0)
    bess_energy_cols = [c for c in df.columns if c.startswith('bess_energy_bus_') and c.endswith('_mwh')]
    baseline_initial_energy_mwh = None
    # Collect full baseline energy trajectory (aggregate across BESS buses) for anchoring SoC path
    baseline_bess_energy_total_series = None
    if bess_energy_cols:
        try:
            baseline_initial_energy_mwh = float(np.sum([df[c].iloc[0] for c in bess_energy_cols]))
            # Sum per timestep to reconstruct aggregate baseline energy trajectory
            baseline_bess_energy_total_series = df[bess_energy_cols].sum(axis=1).to_numpy(dtype=float)
        except Exception:
            baseline_initial_energy_mwh = None
            baseline_bess_energy_total_series = None
        if baseline_initial_energy_mwh is not None and baseline_initial_energy_mwh > 0:
            print(f"[v3] Detected baseline aggregate BESS energy at t=0 = {baseline_initial_energy_mwh:.4f} MWh from v2 export.")
    else:
        baseline_initial_energy_mwh = None

    # BESS meta from v2 (capacity MWh, initial SOC fraction, efficiency)
    try:
        bess_eff = _get_meta_float('meta_bess_eff', 0.95)
    except Exception:
        bess_eff = 0.95
    try:
        bess_initial_soc = _get_meta_float('meta_bess_initial_soc', 0.5)
    except Exception:
        bess_initial_soc = 0.5
    # total capacity across all BESS units (MWh)
    try:
        total_bess_capacity_mwh = _get_meta_float('bess_total_capacity_mwh', np.nan)
        if not np.isfinite(total_bess_capacity_mwh) or total_bess_capacity_mwh <= 0:
            raise ValueError()
    except Exception:
        # fallback: per-unit capacity if present multiplied by number of BESS buses
        try:
            per_bess_cap = _get_meta_float('meta_bess_capacity_mwh', np.nan)
        except Exception:
            per_bess_cap = np.nan
        if not np.isfinite(per_bess_cap) or per_bess_cap <= 0:
            # final fallback guess: 0.25 MWh per BESS bus
            per_bess_cap = 0.25
        total_bess_capacity_mwh = per_bess_cap * (len(bess_caps_by_bus) if bess_caps_by_bus else 0)

    # Precompute line admittances and limits for loading eval
    sqrt3 = float(np.sqrt(3.0))
    Sbase = float(net.sn_mva)  # MVA
    line_data = []  # tuples: (from_bus, to_bus, y_pu, Imax_kA, Vbase_kV_from)
    for line in net.line.itertuples():
        fb = int(line.from_bus); tb = int(line.to_bus)
        Vbase_kV = float(net.bus.at[fb, 'vn_kv'])
        Z_base = (Vbase_kV ** 2) / Sbase
        r_pu = float(line.r_ohm_per_km) * float(line.length_km) / Z_base
        x_pu = float(line.x_ohm_per_km) * float(line.length_km) / Z_base
        z = complex(r_pu, x_pu)
        if abs(z) < 1e-12:
            continue
        y = 1.0 / z
        Imax_kA = float(getattr(line, 'max_i_ka', 0.0))
        line_data.append((fb, tb, y, Imax_kA, Vbase_kV))

    # Precompute transformer admittances and ratings for loading eval (LV side)
    trafo_data = []  # tuples: (hv_bus, lv_bus, y_tr_pu, sn_mva_tr)
    if hasattr(net, 'trafo') and len(net.trafo) > 0:
        for tr in net.trafo.itertuples():
            hv = int(tr.hv_bus); lv = int(tr.lv_bus)
            sn_tr = float(tr.sn_mva) if hasattr(tr, 'sn_mva') else Sbase
            vk_pu = float(tr.vk_percent) / 100.0 if hasattr(tr, 'vk_percent') else 0.05
            r_pu = float(tr.vkr_percent) / 100.0 if hasattr(tr, 'vkr_percent') else 0.01
            x_sq = max(vk_pu**2 - r_pu**2, 1e-8)
            x_pu = float(np.sqrt(x_sq))
            z_tr_pu_own = complex(r_pu, x_pu)
            z_tr_pu_sys = z_tr_pu_own * (Sbase / max(sn_tr, 1e-6))
            if abs(z_tr_pu_sys) < 1e-12:
                continue
            y_tr = 1.0 / z_tr_pu_sys
            trafo_data.append((hv, lv, y_tr, sn_tr))

    # Column groups in samples
    pv_bus_cols = [c for c in pv_df.columns if c.startswith('pv_bus_') and c.endswith('_mw')]
    hp_resid_cols = [c for c in hp_df.columns if c.startswith('hp_residual_bus_') and c.endswith('_mw')]

    per_traj_summary: List[Dict[str, float]] = []

    # Settlement & RT cost factors (DA + imbalance model)
    IMB_UP_FACTOR = 1.3  # premium multiplier for upward imbalance energy (deficit)
    IMB_DN_FACTOR = 1.3  # premium multiplier for downward imbalance energy (surplus absorption)
    PV_CURT_PRICE_FACTOR = 1.0  # opportunity cost factor for curtailed scheduled PV (if any)
    BESS_THROUGHPUT_COST_EUR_PER_MWH = 0.5  # optional degradation proxy

    # Day-ahead energy cost (constant across trajectories): only pay for DA scheduled imports
    da_import_mw = np.maximum(base_net_import, 0.0)
    try:
        da_energy_cost_eur_const = float(np.sum(da_import_mw * price * dt_hours))
    except Exception:
        da_energy_cost_eur_const = 0.0

    # Precompute constant Q factors
    qfactor_household_const = float(np.tan(np.arccos(0.98)))
    qfactor_hp_const = float(np.tan(np.arccos(0.99)))

    # Prepare transformer loading logging
    trafo_loading_buffer: List[tuple] = []  # (sample_id, t_idx, trafo_index, loading_pct_float32)
    trafo_meta_written = False
    trafo_meta_records: List[Dict[str, object]] = []
    trafo_count = len(trafo_data)
    if LOG_TRAFO_LOADING and trafo_count > 0:
        os.makedirs(os.path.join(OUTDIR, TRAFO_LOADING_DIR_NAME), exist_ok=True)
        # Build meta once from trafo_data order
        for idx_tr, (hv, lv, y_tr, sn_tr) in enumerate(trafo_data):
            trafo_meta_records.append({
                'trafo_index': idx_tr,
                'hv_bus': int(hv),
                'lv_bus': int(lv),
                'sn_mva': float(sn_tr)
            })

    # Collect SoC trajectories for envelope export
    collect_soc_frac: List[np.ndarray] = []

    # Determine DRCC mode label early so it's available for per-trajectory series naming
    try:
        base_v2_name = os.path.basename(v2_csv) if v2_csv else ''
    except Exception:
        base_v2_name = ''
    if 'drcc_true' in base_v2_name:
        drcc_mode = 'drcc_true'
    elif 'drcc_false' in base_v2_name:
        drcc_mode = 'drcc_false'
    else:
        drcc_mode = 'drcc_unspecified'

    total = len(sample_ids)
    report_step = max(1, total // 20)  # ~5% steps
    for i, sid in enumerate(sample_ids, start=1):
        if i % report_step == 0 or i == 1 or i == total:
            print(f"[v3] processing sample {i}/{total} ({int(100*i/total)}%)...", flush=True)
        # Slice samples
        temp_sid = temp_df[temp_df['sample_id'] == sid].sort_values('timestamp')
        pv_sid = pv_df[pv_df['sample_id'] == sid].sort_values('timestamp')
        hp_sid = hp_df[hp_df['sample_id'] == sid].sort_values('timestamp')

        # Aggregate sampled PV availability and HP residuals
        pv_avail_rt = pv_sid[pv_bus_cols].to_numpy(dtype=float) if pv_bus_cols else np.zeros((len(index), 0))
        hp_resid_rt = hp_sid[hp_resid_cols].to_numpy(dtype=float) if hp_resid_cols else np.zeros((len(index), 0))
        pv_total_rt = pv_avail_rt.sum(axis=1) if pv_avail_rt.size else np.zeros(len(index))
        hp_resid_total = hp_resid_rt.sum(axis=1) if hp_resid_rt.size else np.zeros(len(index))
        if IGNORE_HP_RESIDUAL:
            hp_resid_total[:] = 0.0  # discard residual noise component

        # Build DA per-step data for recourse
        # Compute temperature-driven HP prediction for this sample using v2 predictor
        # Daily mean temperature per date (C) for the sample
        temp_sid = temp_sid.copy()
        temp_sid['date'] = pd.to_datetime(temp_sid['timestamp']).dt.date
        daily_mean_temp = temp_sid.groupby('date')['temperature_c'].mean().to_dict()
        # Predicted per-bus HP (same value per HP bus as in v2), then aggregate across HP buses
        # Option A: per-device prediction scaled by number of active HP devices (hp_bus_ids_final)
        n_hp = max(1, len(hp_bus_ids_final))
        hp_pred_total = np.zeros(len(index))
        for t_idx, ts in enumerate(index):
            T_C_t = float(temp_sid['temperature_c'].iloc[t_idx]) if t_idx < len(temp_sid) else float('nan')
            if not np.isfinite(T_C_t):
                T_C_t = 20.0
            price_t = float(price[t_idx]) if t_idx < len(price) else 0.0
            HDD_t = max(0.0, HP_PRED_TBASE_C - T_C_t)
            T_avg_d = float(daily_mean_temp.get(ts.date(), T_C_t))
            tod = ts.hour + ts.minute / 60.0
            sin24 = np.sin(2.0 * np.pi * tod / 24.0)
            cos24 = np.cos(2.0 * np.pi * tod / 24.0)
            y_dev = (
                HP_COEFF_B0
                + HP_COEFF_BHDD * HDD_t
                + HP_COEFF_BPI * price_t
                + HP_COEFF_BTAV * T_avg_d
                + HP_COEFF_A1 * sin24
                + HP_COEFF_A2 * cos24
            )
            P_t = baseline_lookup(ts) + HP_PRED_PMAX * y_dev
            P_t = min(max(P_t, 0.0), HP_PRED_PMAX)
            hp_pred_total[t_idx] = P_t * n_hp

        # Sanity check: if predicted total is implausibly high vs DA schedule, warn
        if base_hp_by_bus:
            da_hp_total = np.sum(list(base_hp_by_bus.values()), axis=0)
            da_mean = float(np.mean(da_hp_total)) if len(da_hp_total) else 0.0
            pred_mean = float(np.mean(hp_pred_total)) if len(hp_pred_total) else 0.0
            if da_mean > 0 and pred_mean > 5 * da_mean:
                print(f"  [HP-Scaling Warning] Predictor mean {pred_mean:.4f} MW is >5 DA mean {da_mean:.4f} MW. Check coefficients or baseline.")

        # DA total HP from v2 (aggregate over finalized HP device buses)
        da_hp_total = np.sum([base_hp_by_bus.get(b, np.zeros(len(index))) for b in base_hp_by_bus.keys()], axis=0) if base_hp_by_bus else np.zeros(len(index))
        hp_temp_dev_total = hp_pred_total - da_hp_total

        # Components of deviation per trajectory (arrays length = horizon)
        hp_dev = hp_temp_dev_total + hp_resid_total
        pv_dev = base_pv_da_total - pv_total_rt  # signed PV deviation vs DA schedule (for reporting)
        base_residual = hp_dev + pv_dev          # schedule-based residual
        # Mean-centered policy residual if enabled (and now also used for settlement & constraint evaluation):
        #   hp mean deviation = -hp_resid_total  (hp_pred_total acts as mean predictor)
        #   pv mean deviation = pv_mean_total - pv_total_rt
        #   residual_policy_array = hp_mean_dev + pv_mean_dev
        # If flag is off we revert to schedule-based residual (legacy behavior).
        if USE_MEAN_CENTERED_POLICY:
            pv_mean_dev = pv_mean_total - pv_total_rt
            residual_policy_array = (-hp_resid_total) + pv_mean_dev
        else:
            residual_policy_array = base_residual  # schedule-based residual

        # --- Diagnostics: residual statistics & K*mu shift (printed once for first trajectory) ---
        if i == 1:  # only for first trajectory to avoid log spam
            mean_base = float(np.mean(base_residual))
            mean_policy = float(np.mean(residual_policy_array))
            std_base = float(np.std(base_residual))
            std_policy = float(np.std(residual_policy_array))
            l2_diff = float(np.linalg.norm(residual_policy_array - base_residual))
            print(f"[resid-diag] base_mean={mean_base:.6f} policy_mean={mean_policy:.6f} | base_std={std_base:.6f} policy_std={std_policy:.6f} | l2_diff={l2_diff:.6f}")
            mu_shift = mean_base - mean_policy
            row0 = df.iloc[0]
            lam_p = float(row0.get('lambda_plus', 0.0)); lam_m = float(row0.get('lambda_minus', 0.0))
            chi_m = float(row0.get('chi_minus', 0.0))
            if mean_base >= 0:
                k_mu_bess = lam_p * max(0.0, mu_shift)
                k_mu_curt = 0.0
            else:
                k_mu_bess = - lam_m * max(0.0, -mu_shift)
                k_mu_curt = chi_m * max(0.0, -mu_shift)
            print(f"[resid-diag] mu_shift={mu_shift:.6f} -> approx BESS={k_mu_bess:.6f} MW, Curtail={k_mu_curt:.6f} MW (heuristic)")

        # Accumulators for metrics and voltages (per trajectory, inside sample loop)
        net_import_rt = np.zeros(len(index))
        v_min = float('inf'); v_max = float('-inf')
        steps_voltage_violation = 0
        max_line_loading = 0.0; max_trafo_loading = 0.0
        max_trafo_loading_optionA = 0.0
        steps_line_over_thresh = 0; steps_trafo_over_thresh = 0
        pv_curtail_energy = 0.0; bess_rt_energy_throughput = 0.0
        pv_rt_curt_cost_eur = 0.0; imbalance_cost_eur = 0.0; bess_cycle_cost_eur = 0.0

        # Initialize cumulative recourse adjustment (delta energy) separate from baseline energy path.
        if baseline_initial_energy_mwh is not None and np.isfinite(baseline_initial_energy_mwh) and baseline_bess_energy_total_series is not None:
            # Anchor: baseline energy trajectory gives E_base(t); we track only deviations.
            recourse_energy_delta_mwh = 0.0
            # Provide debug on baseline extrema for context
        else:
            # Fallback: treat baseline energy unknown; start from meta initial SoC * capacity and integrate full schedule+recourse (previous behavior)
            recourse_energy_delta_mwh = None  # flag: None -> use legacy integration branch below
            E_mwh_legacy = max(0.0, min(1.0, bess_initial_soc)) * max(0.0, total_bess_capacity_mwh)
        bess_soc_clip_steps = 0; u_plus_energy = 0.0; u_minus_energy = 0.0
        # Track SoC fraction series for this trajectory
        soc_series = np.zeros(len(index), dtype=float)

        # Collect policy residual stats if enabled
        residual_policy_vals = [] if USE_MEAN_CENTERED_POLICY else None

        # Precompute per-line (parent,child)->(r_pu,x_pu) for voltage drop (once per trajectory)
        # We treat transformers separately using their series impedance (derived earlier) approximated to (r_pu,x_pu)
        line_rx_map: dict[tuple[int,int], tuple[float,float]] = {}
        Sbase_sys = float(net.sn_mva)
        for line in net.line.itertuples():
            fb = int(line.from_bus); tb = int(line.to_bus)
            Vbase_kV = float(net.bus.at[fb, 'vn_kv'])
            Z_base = (Vbase_kV ** 2) / Sbase_sys
            r_pu = float(line.r_ohm_per_km) * float(line.length_km) / Z_base
            x_pu = float(line.x_ohm_per_km) * float(line.length_km) / Z_base
            line_rx_map[(fb, tb)] = (r_pu, x_pu)
        # Transformers: derive (r,x) from vk% and vkr% on system base
        for tr in net.trafo.itertuples():
            hv = int(tr.hv_bus); lv = int(tr.lv_bus)
            vk_pu = float(tr.vk_percent) / 100.0 if hasattr(tr, 'vk_percent') else 0.05
            r_pu = float(tr.vkr_percent) / 100.0 if hasattr(tr, 'vkr_percent') else 0.01
            x_sq = max(vk_pu**2 - r_pu**2, 1e-8)
            x_pu = float(np.sqrt(x_sq))
            # Scale by (Sbase / sn_mva) to put on system base
            sn_tr = float(tr.sn_mva) if hasattr(tr, 'sn_mva') else Sbase_sys
            scale = Sbase_sys / max(sn_tr, 1e-6)
            line_rx_map[(hv, lv)] = (r_pu * scale, x_pu * scale)

        # Build outward traversal order from slack for voltage propagation
        from collections import deque
        bfs_order: List[int] = []
        q = deque([slack_bus_index])
        seen = {slack_bus_index}
        while q:
            b = q.popleft()
            bfs_order.append(b)
            for ch in downstream_map.get(b, []):
                if ch not in seen:
                    seen.add(ch)
                    q.append(ch)

        for t, ts in enumerate(index):
            row = df.iloc[t]
            pv_sched_t = float(base_pv_da_total[t])
            # Residual used for BOTH policy activation and settlement/constraint evolution:
            # When mean-centering is active this is the centered residual; otherwise schedule-based.
            resid_t = float(residual_policy_array[t])

            # Residual signal for policy activation (identical to settlement residual now),
            # keep collecting series only when flag enabled for diagnostics.
            if USE_MEAN_CENTERED_POLICY:
                resid_policy_t = resid_t
                residual_policy_vals.append(resid_policy_t)
            else:
                resid_policy_t = resid_t

            # Policy open-loop commands from affine recourse (based on chosen residual)
            p_bess_cmd, extra_curt_cmd = apply_recourse_for_step(resid_policy_t, row, pv_sched_t)

            if recourse_energy_delta_mwh is not None and baseline_bess_energy_total_series is not None:
                # Anchor mode: baseline energy known exactly; only integrate recourse adjustment
                E_base_t = float(baseline_bess_energy_total_series[t]) if t < len(baseline_bess_energy_total_series) else 0.0
                E_available = E_base_t + recourse_energy_delta_mwh
                E_available = max(0.0, min(total_bess_capacity_mwh, E_available)) if total_bess_capacity_mwh > 0 else E_available

                # Project affine command onto feasible band given current adjusted energy
                p_cmd = float(p_bess_cmd)
                if bess_pmax_total > 0:
                    p_cmd = max(-bess_pmax_total, min(bess_pmax_total, p_cmd))
                else:
                    p_cmd = 0.0
                if dt_hours > 0 and bess_eff > 0 and total_bess_capacity_mwh > 0:
                    p_max_energy = (E_available * bess_eff) / dt_hours
                    charge_headroom = max(0.0, total_bess_capacity_mwh - E_available)
                    p_min_energy = - (charge_headroom / (bess_eff * dt_hours))
                    p_min_total = max(-bess_pmax_total, p_min_energy)
                    p_max_total = min(bess_pmax_total, p_max_energy)
                    if p_min_total > p_max_total:
                        p_min_total, p_max_total = 0.0, 0.0
                    p_real = min(max(p_cmd, p_min_total), p_max_total)
                else:
                    p_real = p_cmd if bess_pmax_total > 0 else 0.0
                # Update cumulative recourse energy delta (baseline untouched)
                if dt_hours > 0 and bess_eff > 0 and total_bess_capacity_mwh > 0:
                    recourse_energy_delta_mwh = recourse_energy_delta_mwh + max(0.0, -p_real) * bess_eff * dt_hours - max(0.0, p_real) / bess_eff * dt_hours
                    # Clip cumulative deviation to keep total energy within [0, cap]
                    E_post = E_base_t + recourse_energy_delta_mwh
                    if E_post < -1e-9 or E_post > total_bess_capacity_mwh + 1e-9:
                        bess_soc_clip_steps += 1
                        E_post = max(0.0, min(total_bess_capacity_mwh, E_post))
                        # Recompute deviation after clipping
                        recourse_energy_delta_mwh = E_post - E_base_t
                else:
                    if abs(p_real) > 1e-9:
                        bess_soc_clip_steps += 1
                soc_series[t] = ((E_base_t + recourse_energy_delta_mwh) / total_bess_capacity_mwh) if total_bess_capacity_mwh > 0 else 0.0
                bess_delta = p_real
                resid_t -= bess_delta
                if abs(p_real - p_bess_cmd) > 1e-9:
                    bess_soc_clip_steps += 1
            else:
                # Legacy fallback branch (no baseline energy path available): integrate full schedule + recourse as before
                if t == 0 and baseline_initial_energy_mwh is not None:
                    print("[v3][warn] Baseline energy series unavailable; using legacy integration path (SoC scaling may deviate).")
                # Reconstruct E_mwh_legacy variable to persist between steps
                if 'E_mwh_legacy' not in locals():
                    E_mwh_legacy = max(0.0, min(1.0, bess_initial_soc)) * max(0.0, total_bess_capacity_mwh)
                # Apply baseline power if available
                if baseline_bess_total_series is not None and dt_hours > 0 and bess_eff > 0 and total_bess_capacity_mwh > 0:
                    p_base_bess_total = float(baseline_bess_total_series[t]) if t < len(baseline_bess_total_series) else 0.0
                    E_mwh_legacy = E_mwh_legacy + max(0.0, -p_base_bess_total) * bess_eff * dt_hours - max(0.0, p_base_bess_total) / bess_eff * dt_hours
                    E_mwh_legacy = max(0.0, min(total_bess_capacity_mwh, E_mwh_legacy))
                # Recourse projection
                p_cmd = float(p_bess_cmd)
                if bess_pmax_total > 0:
                    p_cmd = max(-bess_pmax_total, min(bess_pmax_total, p_cmd))
                else:
                    p_cmd = 0.0
                if dt_hours > 0 and bess_eff > 0 and total_bess_capacity_mwh > 0:
                    p_max_energy = (E_mwh_legacy * bess_eff) / dt_hours
                    charge_headroom = max(0.0, total_bess_capacity_mwh - E_mwh_legacy)
                    p_min_energy = - (charge_headroom / (bess_eff * dt_hours))
                    p_min_total = max(-bess_pmax_total, p_min_energy)
                    p_max_total = min(bess_pmax_total, p_max_energy)
                    if p_min_total > p_max_total:
                        p_min_total, p_max_total = 0.0, 0.0
                    p_real = min(max(p_cmd, p_min_total), p_max_total)
                    E_mwh_legacy = E_mwh_legacy + max(0.0, -p_real) * bess_eff * dt_hours - max(0.0, p_real) / bess_eff * dt_hours
                    E_mwh_legacy = max(0.0, min(total_bess_capacity_mwh, E_mwh_legacy))
                else:
                    p_real = p_cmd if bess_pmax_total > 0 else 0.0
                soc_series[t] = (E_mwh_legacy / total_bess_capacity_mwh) if total_bess_capacity_mwh > 0 else 0.0
                bess_delta = p_real
                resid_t -= bess_delta
                if abs(p_real - p_bess_cmd) > 1e-9:
                    bess_soc_clip_steps += 1

            # --- PV curtailment after battery action (only for remaining surplus) ---
            extra_curt_used = 0.0
            if resid_t < 0 and extra_curt_cmd > 0:
                pv_avail_t = float(pv_total_rt[t])  # real-time available PV (aggregate)
                extra_curt_used = min(extra_curt_cmd, pv_avail_t)
                resid_t += extra_curt_used  # curtail reduces injection (moves residual toward zero)

            # Final PCC imbalance after local actions (signed residual -> two-sided imbalance)
            u_plus = max(0.0, resid_t)
            u_minus = max(0.0, -resid_t)

            # Realized net import (DA base + imbalance settlement)
            net_import_rt[t] = base_net_import[t] + u_plus - u_minus

            # Metrics accumulation
            pv_curtail_energy += float(extra_curt_used) * dt_hours
            bess_rt_energy_throughput += float(abs(bess_delta)) * dt_hours
            u_plus_energy += float(u_plus) * dt_hours
            u_minus_energy += float(u_minus) * dt_hours

            # Costs (DA energy already accounted once; here only RT recourse premiums/opportunity costs)
            p_price = float(price[t]) if t < len(price) else 0.0
            pv_rt_curt_cost_eur += float(extra_curt_used) * PV_CURT_PRICE_FACTOR * p_price * dt_hours
            imbalance_cost_eur += (IMB_UP_FACTOR * float(u_plus) + IMB_DN_FACTOR * float(u_minus)) * p_price * dt_hours
            bess_cycle_cost_eur += float(abs(bess_delta)) * float(BESS_THROUGHPUT_COST_EUR_PER_MWH) * dt_hours

            # Build P/Q injections per bus for LDF & capture component-wise powers
            P_inj = {b: 0.0 for b in net.bus.index}
            Q_inj = {b: 0.0 for b in net.bus.index}
            comp_load = {}   # non-flex base load MW (positive consumption recorded as +, will subtract in injection)
            comp_flex = {}
            comp_hp = {}
            comp_pv = {}
            comp_bess = {}

            # 1) Inject baseline BESS schedule first (before recourse delta) if available
            if REPLAY_V2_BESS_SCHEDULE and baseline_bess_p_by_bus:
                # If multiple BESS buses present, use per-bus series directly; no redistribution needed.
                for b, series in baseline_bess_p_by_bus.items():
                    if t < len(series):
                        p_base_bess = float(series[t])
                        if abs(p_base_bess) > 0.0:
                            comp_bess[b] = comp_bess.get(b, 0.0) + p_base_bess
                            P_inj[b] += p_base_bess  # discharge positive, charge negative already encoded

            # (Recourse delta will be layered later after other components are added.)

            if LOAD_LOAD_VALUES_FROM_V2:
                # Use direct P/Q from v2 results
                for b, series in nonflex_p_by_bus.items():
                    p_mw = float(series[t]) if t < len(series) else 0.0
                    q_mvar = float(nonflex_q_by_bus.get(b, np.zeros(0))[t]) if t < len(nonflex_q_by_bus.get(b, [])) else 0.0
                    if abs(p_mw) > 0.0:
                        comp_load[b] = comp_load.get(b, 0.0) + p_mw
                        P_inj[b] -= p_mw
                        Q_inj[b] -= q_mvar
                for b, series in flex_p_by_bus.items():
                    p_mw = float(series[t]) if t < len(series) else 0.0
                    q_mvar = float(flex_q_by_bus.get(b, np.zeros(0))[t]) if t < len(flex_q_by_bus.get(b, [])) else 0.0
                    if abs(p_mw) > 0.0:
                        comp_flex[b] = comp_flex.get(b, 0.0) + p_mw
                        P_inj[b] -= p_mw
                        Q_inj[b] -= q_mvar
            else:
                # Legacy reconstruction path from profiles / realized flex schedule
                # Non-flex loads
                for load in net.load.itertuples():
                    b = load.bus; name = getattr(load, 'name', '')
                    p_kw = float(elec_ts_map.get(name, np.zeros(len(index)))[t]) if name in elec_ts_map else 0.0
                    p_mw = p_kw / 1000.0
                    comp_load[b] = comp_load.get(b, 0.0) + p_mw
                    P_inj[b] -= p_mw
                    Q_inj[b] -= p_mw * qfactor_household_const

                # Flexible loads (day-ahead realized schedule after curtailment is fixed across OOS)
                if flex_realized_by_bus:
                    for b, series in flex_realized_by_bus.items():
                        if t < len(series):
                            p_flex = float(series[t])
                            if abs(p_flex) > 0.0:
                                comp_flex[b] = comp_flex.get(b, 0.0) + p_flex
                                P_inj[b] -= p_flex
                                Q_inj[b] -= p_flex * qfactor_household_const

            # Heat pumps: use DA hp_elec_bus_*_mw plus residual split proportionally across HP buses
            if base_hp_by_bus:
                # Distribute total HP deviation equally across detected HP devices (Option A)
                total_dev_t = float(hp_temp_dev_total[t] + hp_resid_total[t])
                per_bus_dev = total_dev_t / max(1, len(base_hp_by_bus))
                for b in base_hp_by_bus.keys():
                    p_da = float(base_hp_by_bus.get(b, np.zeros(len(index)))[t])
                    adj_p = p_da + per_bus_dev
                    comp_hp[b] = comp_hp.get(b, 0.0) + adj_p
                    P_inj[b] -= adj_p
                    Q_inj[b] -= adj_p * qfactor_hp_const

            # PV injections: realized PV after extra curtail; distribute proportionally to installed caps
            if pv_bus_ids:
                total_cap = sum(pv_caps_by_bus.get(b, 0.0) for b in pv_bus_ids)
                pv_effective = max(0.0, pv_total_rt[t] - extra_curt_used)
                for b in pv_bus_ids:
                    share = (pv_caps_by_bus.get(b, 0.0) / total_cap) if total_cap > 0 else 0.0
                    p_real_pv = pv_effective * share
                    comp_pv[b] = comp_pv.get(b, 0.0) + p_real_pv
                    P_inj[b] += p_real_pv

            # BESS: implement delta at dominant BESS bus (or distribute)
            if bess_pmax_total > 0 and bess_caps_by_bus:
                # allocate by capacity share
                # If we already placed baseline schedule per bus, add delta on top proportionally by capacity
                for b, cap in bess_caps_by_bus.items():
                    share = cap / bess_pmax_total if bess_pmax_total > 0 else 0.0
                    p_bess_bus_delta = bess_delta * share
                    if abs(p_bess_bus_delta) > 0.0:
                        comp_bess[b] = comp_bess.get(b, 0.0) + p_bess_bus_delta
                        P_inj[b] += p_bess_bus_delta

            # Accumulate once per timestep for radial flows
            P_vec = np.array([P_inj[b] for b in net.bus.index], dtype=float)
            Q_vec = np.array([Q_inj[b] for b in net.bus.index], dtype=float)
            P_acc, Q_acc = accumulate_flows(P_vec, Q_vec)

            # Voltage reconstruction (approximate DistFlow linear drop) if enabled
            if VOLTAGE_EVAL_ENABLED:
                Vbus = {slack_bus_index: VOLTAGE_SLACK_PU}
                # Propagate parent -> children using subtree P/Q at child
                for parent in bfs_order:
                    Vp = Vbus.get(parent, VOLTAGE_SLACK_PU)
                    for child in downstream_map.get(parent, []):
                        if child in Vbus:  # already set
                            continue
                        r_x = line_rx_map.get((parent, child))
                        if r_x is None:
                            # If no direct mapping (shouldn't happen in radial assumption) inherit parent voltage
                            Vbus[child] = Vp
                            continue
                        r_pu, x_pu = r_x
                        # Subtree power at child in per-unit
                        P_pu = P_acc[child] / Sbase_sys
                        Q_pu = Q_acc[child] / Sbase_sys
                        # Linear voltage drop approximation (ignore losses): V_child = V_parent - (r*P + x*Q)
                        Vc = Vp - (r_pu * P_pu + x_pu * Q_pu)
                        Vbus[child] = Vc
                # Record min/max & violations for this timestep
                if Vbus:
                    vt = np.array(list(Vbus.values()), dtype=float)
                    # Filter out any extreme numerical artifacts
                    vt = vt[np.isfinite(vt)]
                    if vt.size:
                        v_min = min(v_min, float(vt.min()))
                        v_max = max(v_max, float(vt.max()))
                        # Thresholds (tight band if provided)
                        v_lo = VOLTAGE_TIGHT_MIN_PU if VOLTAGE_TIGHT_MIN_PU is not None else VOLTAGE_MIN_PU
                        v_hi = VOLTAGE_TIGHT_MAX_PU if VOLTAGE_TIGHT_MAX_PU is not None else VOLTAGE_MAX_PU
                        violated = (vt < v_lo) | (vt > v_hi)
                        if COUNT_VIOLATION_ON_ANY_BUS:
                            if np.any(violated):
                                steps_voltage_violation += 1
                        else:
                            steps_voltage_violation += int(np.sum(violated))
            else:
                # Voltage evaluation disabled; keep placeholders
                pass

            # Line loading (radial): flow on line = apparent power of downstream subtree at child bus
            any_line_over_thresh = False
            # Prepare per-step series containers if exporting
            if EXPORT_FULL_SERIES:
                if 'series_rows' not in locals():
                    series_rows = []
                step_record = {
                    'timestamp': ts,
                    'sample_id': sid,
                    't_index': t,
                    'u_plus_mw': u_plus,
                    'u_minus_mw': u_minus,
                    'residual_policy_mw': resid_policy_t,
                    'residual_post_actions_mw': resid_t,
                    'bess_power_cmd_mw': p_bess_cmd,
                    'bess_power_real_mw': p_real,
                    'pv_extra_curtail_mw': extra_curt_used,
                    'soc_frac': soc_series[t]
                }
            for fb, tb, y, Imax_kA, Vbase_kV in line_data:
                if Imax_kA <= 0:
                    continue
                S_child = np.sqrt(P_acc[tb]**2 + Q_acc[tb]**2)
                # Approx current assuming nominal voltage
                I_est_kA = (S_child) / (sqrt3 * max(Vbase_kV, 1e-6))
                loading_pct = (I_est_kA / Imax_kA) * 100.0
                if loading_pct > max_line_loading:
                    max_line_loading = float(loading_pct)
                if loading_pct > LOADING_VIOLATION_THRESHOLD_PCT:
                    any_line_over_thresh = True
                if EXPORT_FULL_SERIES:
                    step_record[f'line_{fb}_{tb}_loading_pct'] = loading_pct
                    step_record[f'line_{fb}_{tb}_p_mw'] = P_acc[tb]
                    step_record[f'line_{fb}_{tb}_q_mvar'] = Q_acc[tb]
            if any_line_over_thresh:
                steps_line_over_thresh += 1

            # Transformer loading (radial accumulation). OptionA/admittance distinction collapsed.
            any_trafo_over_thresh = False
            for idx_tr, (hv, lv, y_tr, sn_tr) in enumerate(trafo_data):
                S_lv = np.sqrt(P_acc[lv]**2 + Q_acc[lv]**2)
                if sn_tr > 0:
                    loading_pct = (S_lv / sn_tr) * 100.0
                    if loading_pct > max_trafo_loading_optionA:
                        max_trafo_loading_optionA = float(loading_pct)
                    if loading_pct > max_trafo_loading:
                        max_trafo_loading = float(loading_pct)
                    if loading_pct > LOADING_VIOLATION_THRESHOLD_PCT:
                        any_trafo_over_thresh = True
                    if LOG_TRAFO_LOADING:
                        trafo_loading_buffer.append((sid, t, idx_tr, np.float32(loading_pct)))
                    if EXPORT_FULL_SERIES:
                        step_record[f'trafo_{hv}_{lv}_loading_pct'] = loading_pct
                        step_record[f'trafo_{hv}_{lv}_p_mw'] = P_acc[lv]
                        step_record[f'trafo_{hv}_{lv}_q_mvar'] = Q_acc[lv]
            if any_trafo_over_thresh:
                steps_trafo_over_thresh += 1

            if EXPORT_FULL_SERIES:
                # Bus voltages
                if VOLTAGE_EVAL_ENABLED and 'Vbus' in locals():
                    for b, vpu in Vbus.items():
                        step_record[f'bus_{b}_voltage_pu'] = vpu
                # Component powers (store even if zero when present earlier)
                for b, val in comp_load.items():
                    step_record[f'load_bus_{b}_p_mw'] = val
                for b, val in comp_flex.items():
                    step_record[f'flex_bus_{b}_p_mw'] = val
                for b, val in comp_hp.items():
                    step_record[f'hp_bus_{b}_p_mw'] = val
                for b, val in comp_pv.items():
                    step_record[f'pv_bus_{b}_p_mw'] = val
                for b, val in comp_bess.items():
                    step_record[f'bess_bus_{b}_p_mw'] = val
                series_rows.append(step_record)

        # Flush buffer periodically
        if LOG_TRAFO_LOADING and trafo_loading_buffer and (i % TRAFO_LOADING_BUFFER_SAMPLES == 0 or i == total):
            try:
                import pandas as _pd
                buf_df = _pd.DataFrame(trafo_loading_buffer, columns=['sample_id', 't', 'trafo_index', 'loading_pct'])
                # Write/append parquet
                token = _epsilon_token(EPSILON)
                out_dir = os.path.join(OUTDIR, TRAFO_LOADING_DIR_NAME)
                out_path = os.path.join(out_dir, f"{TRAFO_LOADING_FILENAME_PREFIX}{token}.parquet")
                if TRAFO_LOADING_WRITE_PARQUET:
                    # Append mode: if file exists, concat then overwrite (simpler than row-group append without pyarrow writer state)
                    if os.path.exists(out_path):
                        existing = _pd.read_parquet(out_path)
                        buf_df = _pd.concat([existing, buf_df], ignore_index=True)
                    buf_df['loading_pct'] = buf_df['loading_pct'].astype(TRAFO_LOADING_FLOAT_DTYPE)
                    buf_df.to_parquet(out_path, index=False)
                else:
                    # Fallback CSV
                    if os.path.exists(out_path.replace('.parquet', '.csv')):
                        buf_df.to_csv(out_path.replace('.parquet', '.csv'), mode='a', header=False, index=False)
                    else:
                        buf_df.to_csv(out_path.replace('.parquet', '.csv'), index=False)
                trafo_loading_buffer.clear()
            except Exception as e:
                print(f"[WARN] Failed to write transformer loading buffer: {e}")

        # Summarize metrics including voltage & thermal bounds
        summ = _summarize_trajectory(index, price, net_import_rt, dt_hours, pv_total_rt, hp_resid_total, temp_sid['temperature_c'].to_numpy(dtype=float))
        summ['v_min_pu'] = float(v_min) if np.isfinite(v_min) else np.nan
        summ['v_max_pu'] = float(v_max) if np.isfinite(v_max) else np.nan
        summ['max_line_loading_pct'] = max_line_loading
        summ['max_trafo_loading_pct'] = max_trafo_loading
        summ['max_trafo_loading_pct_optionA'] = max_trafo_loading_optionA
        summ['steps_line_over_80pct'] = int(steps_line_over_thresh)
        summ['steps_trafo_over_80pct'] = int(steps_trafo_over_thresh)
        summ['loading_violation_threshold_pct'] = float(LOADING_VIOLATION_THRESHOLD_PCT)
        summ['steps_voltage_violation'] = int(steps_voltage_violation)
        summ['pv_curtail_mwh'] = pv_curtail_energy
        summ['bess_rt_energy_throughput_mwh'] = bess_rt_energy_throughput
        summ['imbalance_mwh'] = float(u_plus_energy + u_minus_energy)
        summ['bess_soc_clip_steps'] = int(bess_soc_clip_steps)
        if USE_MEAN_CENTERED_POLICY and residual_policy_vals:
            rp_arr = np.array(residual_policy_vals, dtype=float)
            summ['residual_policy_std'] = float(np.std(rp_arr))
            summ['residual_policy_frac_negative'] = float(np.mean(rp_arr < 0))
        else:
            summ['residual_policy_std'] = np.nan
            summ['residual_policy_frac_negative'] = np.nan
        # Cost components (DA energy cost constant; store for clarity)
        summ['da_energy_cost_eur'] = float(da_energy_cost_eur_const)
        summ['rt_pv_curtail_cost_eur'] = float(pv_rt_curt_cost_eur)
        summ['rt_imbalance_cost_eur'] = float(imbalance_cost_eur)
        summ['rt_bess_cycle_cost_eur'] = float(bess_cycle_cost_eur)
        summ['total_cost_eur'] = float(da_energy_cost_eur_const + pv_rt_curt_cost_eur + imbalance_cost_eur + bess_cycle_cost_eur)
        summ['sample_id'] = sid
        # Store trajectory length and append (inside loop; previously mis-indented causing only last trajectory retained)
        summ['n_steps'] = int(len(index))
        # Attach static flexible load metrics (same for all trajectories)
        summ.update(flex_metrics_static)
        per_traj_summary.append(summ)
        collect_soc_frac.append(soc_series)

        # Write full series for this trajectory (after loop) to avoid holding all trajectories in memory
    # Only persist heavy per-trajectory series CSV when exactly one trajectory is simulated
    if EXPORT_FULL_SERIES and total == 1 and 'series_rows' in locals() and series_rows:
            os.makedirs(os.path.join(OUTDIR, FULL_SERIES_DIR), exist_ok=True)
            try:
                import pandas as _pd
                series_df = _pd.DataFrame(series_rows)
                # Order columns: timestamp, sample_id, then rest sorted for reproducibility
                fixed_cols = ['timestamp','sample_id','t_index']
                ordered = fixed_cols + [c for c in series_df.columns if c not in fixed_cols]
                series_df = series_df[ordered]
                token = _epsilon_token(EPSILON)
                series_fname = f"trajectory_{drcc_mode}_epsilon_{token}_sample_{sid}.csv" if drcc_mode != 'drcc_false' else f"trajectory_{drcc_mode}_sample_{sid}.csv"
                series_path = os.path.join(OUTDIR, FULL_SERIES_DIR, series_fname)
                series_df.to_csv(series_path, index=False)
                print(f"[series] Wrote full series for sample {sid} -> {series_path}")
            except Exception as e:
                print(f"[WARN] Failed to write full series for sample {sid}: {e}")
            finally:
                del series_rows

    token = _epsilon_token(EPSILON)

    summary_df = pd.DataFrame(per_traj_summary)
    # Filename logic: omit epsilon token for deterministic (drcc_false) case
    if drcc_mode == 'drcc_false':
        summary_filename = 'v3_summary_drcc_false.csv'
    else:
        summary_filename = f'v3_summary_{drcc_mode}_epsilon_{token}.csv'
    summary_path = os.path.join(OUTDIR, summary_filename)
    summary_df.to_csv(summary_path, index=False)

    # === Single-trajectory comprehensive plots ===
    if PLOT_SINGLE_TRAJECTORY_DIAGNOSTICS and len(summary_df) == 1:
        try:
            import matplotlib.pyplot as plt
            import matplotlib.dates as mdates
            traj = summary_df.iloc[0]
            # Locate the full series file we just wrote (if export enabled)
            series_file = None
            if EXPORT_FULL_SERIES:
                token = _epsilon_token(EPSILON)
                pattern_name = f"trajectory_{drcc_mode}_epsilon_{token}_sample_{traj['sample_id']}.csv" if drcc_mode != 'drcc_false' else f"trajectory_{drcc_mode}_sample_{traj['sample_id']}.csv"
                candidate = os.path.join(OUTDIR, FULL_SERIES_DIR, pattern_name)
                if os.path.exists(candidate):
                    series_file = candidate
            series_df = None
            if series_file:
                series_df = pd.read_csv(series_file, parse_dates=['timestamp'])
                series_df = series_df.sort_values('timestamp')
            # Helper plotting utilities
            def _ts(ax, x, y, label=None, **kw):
                ax.plot(x, y, label=label, **kw)
            def _save(fig, name):
                outp = os.path.join(OUTDIR, name)
                fig.tight_layout()
                fig.savefig(outp, dpi=140)
                print(f"[plot] {outp}")
            # 1. Voltages (all buses) if available
            if series_df is not None:
                v_cols = [c for c in series_df.columns if c.startswith('bus_') and c.endswith('_voltage_pu')]
                if v_cols:
                    fig, ax = plt.subplots(figsize=(10,4))
                    for c in v_cols:
                        ax.plot(series_df['timestamp'], series_df[c], linewidth=0.7, alpha=0.7)
                    ax.set_title('Bus Voltages (p.u.)')
                    ax.set_ylabel('p.u.')
                    ax.grid(alpha=0.3)
                    _save(fig, 'singletraj_bus_voltages.png')
            # 2. Transformer loading
            if series_df is not None:
                trafo_cols = [c for c in series_df.columns if c.startswith('trafo_') and c.endswith('_loading_pct')]
                if trafo_cols:
                    fig, ax = plt.subplots(figsize=(10,4))
                    for c in trafo_cols:
                        ax.plot(series_df['timestamp'], series_df[c], linewidth=0.9, alpha=0.8)
                    ax.axhline(LOADING_VIOLATION_THRESHOLD_PCT, color='red', linestyle='--', linewidth=1.0, label='threshold')
                    ax.set_title('Transformer Loading %')
                    ax.set_ylabel('%')
                    ax.legend(fontsize=8)
                    ax.grid(alpha=0.3)
                    _save(fig, 'singletraj_trafo_loading.png')
            # 3. Line loading (may be many; show up to first 40)
            if series_df is not None:
                line_cols = [c for c in series_df.columns if c.startswith('line_') and c.endswith('_loading_pct')]
                if line_cols:
                    subset = line_cols[:40]
                    fig, ax = plt.subplots(figsize=(10,4))
                    for c in subset:
                        ax.plot(series_df['timestamp'], series_df[c], linewidth=0.6, alpha=0.6)
                    ax.axhline(LOADING_VIOLATION_THRESHOLD_PCT, color='red', linestyle='--', linewidth=1.0)
                    ax.set_title(f'Line Loading % (first {len(subset)})')
                    ax.set_ylabel('%')
                    ax.grid(alpha=0.3)
                    _save(fig, 'singletraj_line_loading.png')
            # 4. Component powers stacked (aggregate)
            if series_df is not None:
                t = series_df['timestamp']
                # Build aggregate series from component per-bus columns
                def _agg(prefix):
                    cols = [c for c in series_df.columns if c.startswith(prefix)]
                    if not cols:
                        return np.zeros(len(series_df))
                    return series_df[cols].sum(axis=1).to_numpy()
                p_load = _agg('load_bus_')
                p_flex = _agg('flex_bus_')
                p_hp = _agg('hp_bus_')
                p_pv = _agg('pv_bus_')
                p_bess = _agg('bess_bus_')
                fig, ax = plt.subplots(figsize=(10,4))
                ax.plot(t, p_load, label='Base Load', linewidth=1.1)
                ax.plot(t, p_flex, label='Flex Load', linewidth=1.1)
                ax.plot(t, p_hp, label='HP Elec', linewidth=1.1)
                ax.plot(t, p_pv, label='PV Inject', linewidth=1.1)
                ax.plot(t, p_bess, label='BESS Power', linewidth=1.1)
                ax.set_title('Aggregate Component Powers (MW)')
                ax.set_ylabel('MW')
                ax.grid(alpha=0.3)
                ax.legend(fontsize=8, ncol=3)
                _save(fig, 'singletraj_component_powers.png')

                # --- Diagnostic: compare flex aggregate vs original v2 flex load columns ---
                try:
                    if LOAD_LOAD_VALUES_FROM_V2:
                        v2_flex_cols = [c for c in df.columns if c.startswith('load_flex_p_bus_') and c.endswith('_mw')]
                        if v2_flex_cols:
                            # Ensure we have a pure datetime index
                            if not isinstance(df.index, pd.DatetimeIndex):
                                # Attempt to coerce using 'timestamp' column if present
                                if 'timestamp' in df.columns:
                                    _ts_coerced = pd.to_datetime(df['timestamp'])
                                else:
                                    _ts_coerced = pd.RangeIndex(start=0, stop=len(df), step=1)
                            else:
                                _ts_coerced = df.index
                            v2_sum = df[v2_flex_cols].sum(axis=1).to_numpy()
                            # Align by position (assumes same horizon). If lengths differ, trim to min length.
                            n_common = min(len(v2_sum), len(p_flex))
                            comp_df = pd.DataFrame({
                                'timestamp': t[:n_common].to_numpy(),
                                'flex_v2_sum_mw': v2_sum[:n_common],
                                'flex_v3_sum_mw': p_flex[:n_common]
                            })
                            comp_df['diff_mw'] = comp_df['flex_v3_sum_mw'] - comp_df['flex_v2_sum_mw']
                            comp_df['rel_pct'] = np.where(comp_df['flex_v2_sum_mw'].abs() > 1e-9,
                                                          100.0 * comp_df['diff_mw'] / comp_df['flex_v2_sum_mw'], np.nan)
                            token = _epsilon_token(EPSILON)
                            sample_id = str(traj['sample_id'])
                            out_comp = os.path.join(OUTDIR, f'flex_load_comparison_{token}_sample_{sample_id}.csv')
                            comp_df.to_csv(out_comp, index=False)
                            abs_max = comp_df['diff_mw'].abs().max()
                            energy_diff = float((comp_df['diff_mw'] * dt_hours).sum()) if 'dt_hours' in locals() else float(np.nan)
                            print(f"[diag-flex] Wrote {out_comp} | max_abs_diff={abs_max:.6f} MW | energy_diff={energy_diff:.6f} MWh")
                        else:
                            print("[diag-flex] No v2 load_flex_p_bus_* columns found for comparison.")
                except Exception as _e_diag:
                    print(f"[diag-flex] Failed flex comparison diagnostic: {_e_diag}")
            # 5. SoC time series if envelope file exists (we have only single trajectory SoC series in series_df)
            if series_df is not None and 'soc_frac' in series_df.columns:
                fig, ax = plt.subplots(figsize=(10,3.2))
                ax.plot(series_df['timestamp'], series_df['soc_frac'], color='#1b9e77')
                ax.set_ylim(0, 1.02)
                ax.set_title('BESS State of Charge (fraction)')
                ax.set_ylabel('SoC')
                ax.grid(alpha=0.3)
                _save(fig, 'singletraj_soc.png')
            # 6. Residual & actions
            if series_df is not None and {'residual_policy_mw','residual_post_actions_mw'} <= set(series_df.columns):
                fig, ax = plt.subplots(figsize=(10,3.5))
                ax.plot(series_df['timestamp'], series_df['residual_policy_mw'], label='Policy residual', linewidth=1.0)
                ax.plot(series_df['timestamp'], series_df['residual_post_actions_mw'], label='Post-actions residual', linewidth=1.0)
                ax.set_title('Residual Trajectories (MW)')
                ax.set_ylabel('MW')
                ax.axhline(0, color='black', linewidth=0.8)
                ax.grid(alpha=0.3)
                ax.legend(fontsize=8)
                _save(fig, 'singletraj_residuals.png')
            # 7. Net import vs price (if price present)
            if 'electricity_price_eur_mwh' in df.columns:
                fig, ax1 = plt.subplots(figsize=(10,4))
                ax1.plot(df.index, np.maximum(base_net_import,0.0), label='DA Import MW', color='#4c72b0')
                ax1.set_ylabel('Import MW')
                ax2 = ax1.twinx()
                ax2.plot(df.index, df['electricity_price_eur_mwh'], label='Price EUR/MWh', color='#dd8452', alpha=0.7)
                ax2.set_ylabel('EUR/MWh')
                ax1.set_title('DA Import & Price')
                ax1.grid(alpha=0.3)
                _save(fig, 'singletraj_da_price.png')
        except Exception as e:
            print(f"[WARN] Failed to produce single trajectory plots: {e}")

    # Export SoC envelope (05/50/95) across trajectories with naming consistent to summary/meta
    soc_env_path = None
    if collect_soc_frac:
        try:
            soc_mat = np.vstack(collect_soc_frac)
            soc_p05 = np.percentile(soc_mat, 5, axis=0)
            soc_p50 = np.percentile(soc_mat, 50, axis=0)
            soc_p95 = np.percentile(soc_mat, 95, axis=0)
            soc_env_df = pd.DataFrame({
                'timestamp': index,
                'soc_p05': soc_p05,
                'soc_p50': soc_p50,
                'soc_p95': soc_p95,
            })
            if drcc_mode == 'drcc_false':
                soc_env_filename = 'soc_envelope_drcc_false.csv'
            else:
                soc_env_filename = f"soc_envelope_{drcc_mode}_epsilon_{token}.csv"
            soc_env_path = os.path.join(OUTDIR, soc_env_filename)
            soc_env_df.to_csv(soc_env_path, index=False)
            print(f" SoC envelope written: {soc_env_path}")
        except Exception as e:
            print(f"[WARN] Failed to write SoC envelope: {e}")

    meta = {
        'v2_results_csv': os.path.abspath(v2_csv),
        'epsilon': EPSILON,
        'drcc_mode': drcc_mode,
        'samples_dir': os.path.abspath(SAMPLES_DIR),
        'dt_hours': float(dt_hours),
        'n_trajectories': int(len(summary_df)),
        'metrics': list(summary_df.columns),
        'write_diagnostics': WRITE_DIAGNOSTICS,
        'rt_cost_factors': {
            'imb_up_factor': IMB_UP_FACTOR,
            'imb_dn_factor': IMB_DN_FACTOR,
            'pv_curt_price_factor': PV_CURT_PRICE_FACTOR,
            'bess_throughput_cost_eur_per_mwh': BESS_THROUGHPUT_COST_EUR_PER_MWH
        },
        'ignore_hp_residual': bool(IGNORE_HP_RESIDUAL),
        'use_mean_centered_policy': bool(USE_MEAN_CENTERED_POLICY),
    'trafo_loading_logging': bool(LOG_TRAFO_LOADING),
    'use_optionA_flow': bool(USE_OPTIONA_FLOW),
    'compare_flows': bool(COMPARE_FLOWS)
    }
    # Add residual basis descriptor
    meta['rt_residual_basis'] = 'mean_centered' if USE_MEAN_CENTERED_POLICY else 'schedule'
    if soc_env_path:
        meta['soc_envelope_file'] = os.path.basename(soc_env_path)
    # Policy coefficients export (raw)
    policy_coeffs_path = None
    policy_cols = [c for c in [
        'lambda0_mw','lambda_plus','lambda_minus',
        'chi0_mw','chi_minus',
        'gamma0_mw','gamma_plus',
        'rho_plus_0','rho_plus_1','rho_minus_0','rho_minus_1'
    ] if c in df.columns]
    if policy_cols:
        try:
            policy_df = df[policy_cols].copy()
            policy_df.insert(0, 'timestamp', df.index)
            if drcc_mode == 'drcc_false':
                policy_coeffs_filename = 'policy_coeffs_drcc_false.csv'
            else:
                policy_coeffs_filename = f"policy_coeffs_{drcc_mode}_epsilon_{token}.csv"
            policy_coeffs_path = os.path.join(OUTDIR, policy_coeffs_filename)
            policy_df.to_csv(policy_coeffs_path, index=False)
            print(f" Policy coefficients written: {policy_coeffs_path}")
            meta['policy_coeffs_file'] = os.path.basename(policy_coeffs_path)
        except Exception as e:
            print(f"[WARN] Failed to export policy coefficients: {e}")
    if LOG_TRAFO_LOADING and trafo_meta_records:
        meta['trafo_loading_file'] = os.path.join(TRAFO_LOADING_DIR_NAME, f"{TRAFO_LOADING_FILENAME_PREFIX}{_epsilon_token(EPSILON)}.parquet")
        meta['n_trafos'] = len(trafo_meta_records)
        # Write meta for transformers if not exists
        try:
            meta_path_tr = os.path.join(OUTDIR, TRAFO_LOADING_DIR_NAME, f"trafo_meta_{_epsilon_token(EPSILON)}.csv")
            if not os.path.exists(meta_path_tr):
                pd.DataFrame(trafo_meta_records).to_csv(meta_path_tr, index=False)
        except Exception as e:
            print(f"[WARN] Could not write trafo meta CSV: {e}")
    # Meta filename mirrors summary logic
    if drcc_mode == 'drcc_false':
        meta_filename = 'v3_meta_drcc_false.json'
    else:
        meta_filename = f'v3_meta_{drcc_mode}_epsilon_{token}.json'
    with open(os.path.join(OUTDIR, meta_filename), 'w', encoding='utf-8') as f:
        json.dump(meta, f, indent=2)

    print(f" v3 summary written: {summary_path}")
    print(f" v3 meta written: {os.path.join(OUTDIR, meta_filename)}")


if __name__ == "__main__":
    main()