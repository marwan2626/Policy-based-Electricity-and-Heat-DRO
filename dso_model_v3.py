"""
Out-of-sample Monte Carlo harness (v3): evaluates fixed DA policies from v2 against sampled uncertainties.

How to use (no command-line needed):
- Edit the USER CONFIG block below (EPSILON, RESULTS_CSV, SAMPLES_DIR, MAX_TRAJ, WRITE_DIAGNOSTICS, OUTDIR)
- Save file; then run it.

What it does:
- Loads a v2 results CSV (policy + baseline series) inferred from EPSILON or explicit RESULTS_CSV
- Loads consolidated samples generated by generate_samples.py (temperature, PV availability per bus, HP residual per bus)
- For each trajectory (sample_id), computes realized series (PV, HP residual) and a proxy net import
- Computes energy cost (no buy-back revenue), peak import, energy totals; aggregates per-trajectory metrics
- Optionally writes per-trajectory time series diagnostics

Notes and assumptions:
- DA plan is fixed; we simulate PV availability deviations and HP deviations (temperature-driven predictor delta + residuals)
- Proxy net import: net_import_rt = net_import_da + (hp_temp_dev + hp_residual) - (pv_realized - pv_DA_setpoint)
  This assumes other components stay at DA plan and PV deviations directly affect grid import/export.
- We do NOT apply DRCC network tightenings here; OOS is unconstrained aside from physical clipping already in samples.
- Export buy-back is assumed disabled (cost only for positive imports), consistent with v2 settings.
- Hooks are provided to extend with LDF-based flow/violation checks if sensitivities are available.
"""

from __future__ import annotations

import json
import os
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd


# ===========================
# USER CONFIG (edit these)
# ===========================
# Epsilon used in the v2 filename suffix: dso_model_v2_results_drcc_true_epsilon_{EPSILON_TOKEN}.csv
# The token uses two decimals with underscore as decimal separator, e.g., 0.05 -> "0_05"
EPSILON: float = 0.05
# When running the deterministic (no DRCC tightening) case, set RUN_DRCC_FALSE = True.
# In that mode we ignore EPSILON for locating the v2 results CSV and instead look for
# files named like: dso_model_v2_results_drcc_false*.csv
RUN_DRCC_FALSE: bool = False

# If RESULTS_CSV is None, we'll try to find a file named with the epsilon token in the current folder.
RESULTS_CSV: Optional[str] = None

# Directory containing consolidated samples from generate_samples.py
SAMPLES_DIR: str = "samples"

# Limit number of trajectories (sample_id) for a quick run. None means evaluate all.
MAX_TRAJ: Optional[int] = None

# Toggle to ignore stochastic HP residuals in OOS (Option A). When True, only temperature-driven HP deviations remain.
IGNORE_HP_RESIDUAL: bool = False

# Write per-trajectory time series diagnostics (large files). Disabled by default.
WRITE_DIAGNOSTICS: bool = False

# Output directory for v3 results
OUTDIR: str = "v3_oos"

# Threshold (in percent) above which a line or transformer loading counts as a violation step
LOADING_VIOLATION_THRESHOLD_PCT: float = 80.0

# Phase 1: Use mean-centered residual for policy activation (instead of schedule-referenced residual)
USE_MEAN_CENTERED_POLICY: bool = True  # set False to use schedule-based residual directly

# Logging of transformer loading distributions (for CVaR / tail risk analysis)
LOG_TRAFO_LOADING: bool = True  # set True to enable logging
TRAFO_LOADING_BUFFER_SAMPLES: int = 50  # flush every N samples
TRAFO_LOADING_DIR_NAME: str = "v3_loading"  # subdirectory inside OUTDIR
TRAFO_LOADING_FILENAME_PREFIX: str = "trafo_loading_raw_epsilon_"  # suffix token + .parquet
TRAFO_LOADING_FLOAT_DTYPE = "float32"
TRAFO_LOADING_WRITE_PARQUET: bool = True  # requires pyarrow


# ===========================
# Helpers
# ===========================

def _epsilon_token(eps: float) -> str:
    """Format epsilon as '0_05' (two decimals, dot -> underscore)."""
    s = f"{eps:.2f}"
    return s.replace(".", "_")


def _infer_v2_csv(epsilon: float) -> Optional[str]:
    """Locate a v2 results CSV.

    Logic:
    - If RUN_DRCC_FALSE is True: ignore epsilon and search for deterministic files
      starting with 'dso_model_v2_results_drcc_false'. If multiple, pick most recent.
    - Else (DRCC / tightened case): use epsilon token to find 'drcc_true' file first;
      fallback to any file containing that token.
    """
    if RUN_DRCC_FALSE:
        prefix = "dso_model_v2_results_drcc_false"
        det_candidates = [
            f for f in os.listdir(os.getcwd())
            if f.startswith(prefix) and f.endswith('.csv')
        ]
        if not det_candidates:
            return None
        det_candidates.sort(key=lambda p: os.path.getmtime(p), reverse=True)
        return det_candidates[0]

    # DRCC (tightened) path uses epsilon token
    token = _epsilon_token(epsilon)
    canonical = f"dso_model_v2_results_drcc_true_epsilon_{token}.csv"
    if os.path.exists(canonical):
        return canonical
    # Fallback: any file with the token (keeps backward compatibility if naming shifted)
    candidates = [
        f for f in os.listdir(os.getcwd())
        if f.startswith("dso_model_v2_results_") and f.endswith('.csv') and token in f
    ]
    if candidates:
        candidates.sort(key=lambda p: os.path.getmtime(p), reverse=True)
        return candidates[0]
    return None


def _load_v2_results(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    if 'timestamp' in df.columns:
        ts = pd.to_datetime(df['timestamp'])
        df.index = ts
    else:
        # Try reconstructing from meta_time_start and dt_hours
        try:
            start_raw = df.get('meta_time_start', [None])[0]
            start = pd.to_datetime(start_raw) if start_raw is not None else pd.NaT
        except Exception:
            start = pd.NaT
        try:
            dt_hours = float(df.get('meta_dt_hours', [np.nan])[0])
        except Exception:
            dt_hours = np.nan
        if pd.isna(start) or not np.isfinite(dt_hours):
            # Leave index as RangeIndex; caller may fix from samples
            pass
        else:
            df.index = pd.date_range(start=start, periods=len(df), freq=f"{int(round(dt_hours*60))}min")
    return df


def _load_samples(samples_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    # Temperature (long)
    temp_path = os.path.join(samples_dir, 'samples_temperature_c.csv')
    pv_path = os.path.join(samples_dir, 'samples_pv.csv')
    hp_path = os.path.join(samples_dir, 'samples_hp_residual.csv')
    if not (os.path.exists(temp_path) and os.path.exists(pv_path) and os.path.exists(hp_path)):
        raise FileNotFoundError(
            f"Missing sample files in {samples_dir}. Expected samples_temperature_c.csv, samples_pv.csv, samples_hp_residual.csv"
        )

    temp_df = pd.read_csv(temp_path, parse_dates=['timestamp'])
    pv_df = pd.read_csv(pv_path, parse_dates=['timestamp'])
    hp_df = pd.read_csv(hp_path, parse_dates=['timestamp'])
    return temp_df, pv_df, hp_df


def _load_samples_meta(samples_dir: str) -> Optional[Dict[str, object]]:
    meta_path = os.path.join(samples_dir, 'samples_meta.json')
    if os.path.exists(meta_path):
        try:
            with open(meta_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return None
    return None


def _get_bus_ids_from_columns(df: pd.DataFrame, prefix: str, suffix: str) -> List[int]:
    ids: List[int] = []
    for c in df.columns:
        if c.startswith(prefix) and c.endswith(suffix):
            try:
                mid = c[len(prefix):-len(suffix)]
                ids.append(int(mid))
            except Exception:
                continue
    return sorted(list(set(ids)))


def _summarize_trajectory(
    ts: pd.DatetimeIndex,
    price_eur_mwh: np.ndarray,
    net_import_mw: np.ndarray,
    dt_hours: float,
    pv_total_mw: np.ndarray,
    hp_resid_total_mw: np.ndarray,
    temp_c: Optional[np.ndarray],
) -> Dict[str, float]:
    # Imports (positive only)
    import_mw = np.maximum(net_import_mw, 0.0)
    import_mwh = float(np.sum(import_mw * dt_hours))
    energy_cost_eur = float(np.sum(import_mw * price_eur_mwh * dt_hours))
    peak_import_mw = float(np.max(import_mw)) if len(import_mw) else 0.0
    pv_energy_mwh = float(np.sum(np.maximum(pv_total_mw, 0.0) * dt_hours))
    hp_resid_energy_mwh_abs = float(np.sum(np.abs(hp_resid_total_mw) * dt_hours))
    avg_temp_c = float(np.mean(temp_c)) if temp_c is not None and len(temp_c) else np.nan
    return {
        'import_mwh': import_mwh,
        'energy_cost_eur': energy_cost_eur,
        'peak_import_mw': peak_import_mw,
        'pv_energy_mwh': pv_energy_mwh,
        'hp_resid_energy_mwh_abs': hp_resid_energy_mwh_abs,
        'avg_temp_c': avg_temp_c,
    }


def _collect_da_series(df: pd.DataFrame, bus_ids: List[int], prefix: str, suffix: str) -> Dict[int, np.ndarray]:
    out: Dict[int, np.ndarray] = {}
    for b in bus_ids:
        col = f"{prefix}{b}{suffix}"
        if col in df.columns:
            out[b] = df[col].to_numpy(dtype=float)
    return out


def create_network_from_csv():
    network_data_path = "extracted_network_data"
    electrical_buses = pd.read_csv(os.path.join(network_data_path, "electrical_buses.csv"))
    electrical_lines = pd.read_csv(os.path.join(network_data_path, "electrical_lines.csv"))
    electrical_loads = pd.read_csv(os.path.join(network_data_path, "electrical_loads.csv"))
    electrical_pv_systems = pd.read_csv(os.path.join(network_data_path, "electrical_pv_systems.csv"))
    electrical_bess = pd.read_csv(os.path.join(network_data_path, "electrical_bess.csv"))
    electrical_transformers = pd.read_csv(os.path.join(network_data_path, "electrical_transformers.csv"))
    electrical_external_grids = pd.read_csv(os.path.join(network_data_path, "electrical_external_grids.csv"))

    class NetworkFromCSV:
        def __init__(self):
            self.sn_mva = 100.0
            self.bus = pd.DataFrame({
                'vn_kv': electrical_buses['vn_kv'],
                'name': electrical_buses['name'],
                'type': electrical_buses['type'],
                'zone': electrical_buses.get('zone', ''),
                'in_service': electrical_buses.get('in_service', True)
            })
            self.bus.index = electrical_buses['bus_id'].values
            self.line = pd.DataFrame({
                'from_bus': electrical_lines['from_bus'],
                'to_bus': electrical_lines['to_bus'],
                'length_km': electrical_lines['length_km'],
                'r_ohm_per_km': electrical_lines['r_ohm_per_km'],
                'x_ohm_per_km': electrical_lines['x_ohm_per_km'],
                'c_nf_per_km': electrical_lines.get('c_nf_per_km', 0),
                'max_i_ka': electrical_lines['max_i_ka'],
                'name': electrical_lines['name'],
                'in_service': electrical_lines.get('in_service', True)
            })
            self.trafo = pd.DataFrame({
                'hv_bus': electrical_transformers['hv_bus'],
                'lv_bus': electrical_transformers['lv_bus'],
                'sn_mva': electrical_transformers['sn_mva'],
                'vn_hv_kv': electrical_transformers['vn_hv_kv'],
                'vn_lv_kv': electrical_transformers['vn_lv_kv'],
                'vk_percent': electrical_transformers['vk_percent'],
                'vkr_percent': electrical_transformers['vkr_percent'],
                'name': electrical_transformers['name'],
                'in_service': electrical_transformers.get('in_service', True)
            })
            default_p_mw = 0.001
            default_q_mvar = 0.0
            default_controllable = False
            self.load = pd.DataFrame({
                'bus': electrical_loads['bus'],
                'p_mw': electrical_loads.get('p_mw', default_p_mw),
                'q_mvar': electrical_loads.get('q_mvar', default_q_mvar),
                'controllable': electrical_loads.get('controllable', default_controllable),
                'name': electrical_loads.get('name', 'Load'),
                'in_service': electrical_loads.get('in_service', True)
            })
            self.sgen = pd.DataFrame({
                'bus': electrical_pv_systems['bus_id'],
                'p_mw': electrical_pv_systems['capacity_kw'] / 1000.0,
                'q_mvar': electrical_pv_systems.get('q_mvar', 0.0),
                'name': electrical_pv_systems.get('name', 'PV'),
                'in_service': electrical_pv_systems.get('in_service', True),
                'type': electrical_pv_systems.get('type', 'PV')
            })
            # Add BESS as sgen entries
            bess_sgen = pd.DataFrame()
            if electrical_bess is not None and len(electrical_bess) > 0:
                if 'max_p_mw' in electrical_bess.columns:
                    p_mw_series = electrical_bess['max_p_mw'].astype(float)
                elif 'p_mw' in electrical_bess.columns:
                    p_mw_series = electrical_bess['p_mw'].astype(float)
                elif 'max_p_kw' in electrical_bess.columns:
                    p_mw_series = electrical_bess['max_p_kw'].astype(float) / 1000.0
                elif 'p_kw' in electrical_bess.columns:
                    p_mw_series = electrical_bess['p_kw'].astype(float) / 1000.0
                else:
                    p_mw_series = pd.Series(0.0, index=electrical_bess.index)
                bess_sgen = pd.DataFrame({
                    'bus': electrical_bess['bus'],
                    'p_mw': p_mw_series,
                    'q_mvar': electrical_bess.get('q_mvar', 0.0),
                    'name': electrical_bess.get('name', 'BESS'),
                    'in_service': electrical_bess.get('in_service', True),
                    'type': electrical_bess.get('type', 'BESS')
                })
                try:
                    bess_sgen['bus'] = bess_sgen['bus'].astype(int)
                except Exception:
                    pass
            if bess_sgen is not None and len(bess_sgen) > 0:
                for col in ['bus', 'p_mw', 'q_mvar', 'name', 'in_service', 'type']:
                    if col not in self.sgen.columns:
                        self.sgen[col] = None
                    if col not in bess_sgen.columns:
                        bess_sgen[col] = None
                self.sgen = pd.concat([self.sgen, bess_sgen], ignore_index=True, sort=False).reset_index(drop=True)
            self.ext_grid = pd.DataFrame({
                'bus': electrical_external_grids['bus'],
                'vm_pu': electrical_external_grids.get('vm_pu', 1.0),
                'va_degree': electrical_external_grids.get('va_degree', 0.0),
                'name': electrical_external_grids.get('name', 'External Grid'),
                'in_service': electrical_external_grids.get('in_service', True)
            })
            self.controller = pd.DataFrame(columns=['object'])
    return NetworkFromCSV()


def calculate_gbus_matrix(net):
    num_buses = len(net.bus)
    Gbus = np.zeros((num_buses, num_buses))
    for line in net.line.itertuples():
        from_bus = line.from_bus; to_bus = line.to_bus
        base_voltage = net.bus.at[from_bus, 'vn_kv']
        Z_base = base_voltage ** 2 / net.sn_mva
        x_pu = line.x_ohm_per_km * line.length_km / Z_base
        r_pu = line.r_ohm_per_km * line.length_km / Z_base
        Y_series = 1 / (r_pu + 1j * x_pu)
        g = np.real(Y_series)
        Gbus[from_bus, from_bus] += g
        Gbus[to_bus, to_bus] += g
        Gbus[from_bus, to_bus] -= g
        Gbus[to_bus, from_bus] -= g
    return Gbus


def calculate_bbus_matrix(net):
    num_buses = len(net.bus)
    Bbus = np.zeros((num_buses, num_buses))
    for line in net.line.itertuples():
        from_bus = line.from_bus; to_bus = line.to_bus
        base_voltage = net.bus.at[from_bus, 'vn_kv']
        Z_base = base_voltage ** 2 / net.sn_mva
        x_pu = line.x_ohm_per_km * line.length_km / Z_base
        r_pu = line.r_ohm_per_km * line.length_km / Z_base
        Y_series = 1 / (r_pu + 1j * x_pu)
        b = np.imag(Y_series)
        # Use the imaginary part directly (note: typically negative for inductive lines)
        Bbus[from_bus, from_bus] += b
        Bbus[to_bus, to_bus] += b
        Bbus[from_bus, to_bus] -= b
        Bbus[to_bus, from_bus] -= b
    return Bbus


def build_Ybus(net):
    num_buses = len(net.bus)
    Ybus = np.zeros((num_buses, num_buses), dtype=complex)
    # Lines
    for line in net.line.itertuples():
        fb = int(line.from_bus); tb = int(line.to_bus)
        base_voltage = float(net.bus.at[fb, 'vn_kv'])
        Z_base = base_voltage ** 2 / float(net.sn_mva)
        r_pu = float(line.r_ohm_per_km) * float(line.length_km) / Z_base
        x_pu = float(line.x_ohm_per_km) * float(line.length_km) / Z_base
        z = complex(r_pu, x_pu)
        if abs(z) < 1e-12:
            continue
        y = 1.0 / z
        Ybus[fb, fb] += y
        Ybus[tb, tb] += y
        Ybus[fb, tb] -= y
        Ybus[tb, fb] -= y
    # Transformers (approximate series model on common Sbase)
    if hasattr(net, 'trafo') and len(net.trafo) > 0:
        Sbase_sys = float(net.sn_mva)
        for tr in net.trafo.itertuples():
            hv = int(tr.hv_bus); lv = int(tr.lv_bus)
            sn_tr = float(tr.sn_mva) if hasattr(tr, 'sn_mva') else Sbase_sys
            vk_pu = float(tr.vk_percent) / 100.0 if hasattr(tr, 'vk_percent') else 0.05
            r_pu = float(tr.vkr_percent) / 100.0 if hasattr(tr, 'vkr_percent') else 0.01
            x_sq = max(vk_pu**2 - r_pu**2, 1e-8)
            x_pu = float(np.sqrt(x_sq))
            z_tr_pu_own = complex(r_pu, x_pu)
            # Convert to system Sbase
            z_tr_pu_sys = z_tr_pu_own * (Sbase_sys / max(sn_tr, 1e-6))
            if abs(z_tr_pu_sys) < 1e-12:
                continue
            y_tr = 1.0 / z_tr_pu_sys
            Ybus[hv, hv] += y_tr
            Ybus[lv, lv] += y_tr
            Ybus[hv, lv] -= y_tr
            Ybus[lv, hv] -= y_tr
    return Ybus


def build_electrical_time_series(time_index: pd.DatetimeIndex) -> Dict[str, np.ndarray]:
    profiles_path = 'vdi_profiles/all_house_profiles.csv'
    if not os.path.exists(profiles_path):
        return {}
    profiles_df = pd.read_csv(profiles_path, index_col=0)
    profiles_df.index = pd.to_datetime(profiles_df.index)
    window_df = profiles_df.reindex(time_index).fillna(0.0)

    # Load electrical load definitions
    electrical_loads = pd.read_csv(os.path.join('extracted_network_data', 'electrical_loads.csv'))
    load_names = electrical_loads['name'].astype(str).fillna('')
    def _norm(s: str) -> str:
        return s.replace('.', '_').replace('-', '_').lower()

    cols_norm = {_norm(c): c for c in window_df.columns}
    series_map: Dict[str, np.ndarray] = {}
    for el_orig in load_names:
        if not el_orig:
            continue
        el_norm = _norm(el_orig)
        candidates = [f"{el_norm}_electricity", f"{el_norm}.electricity"]
        chosen = None
        for cand in candidates:
            if cand in cols_norm:
                chosen = cols_norm[cand]; break
        if chosen is None:
            # try swapped separators
            for base in (el_norm.replace('.', '_'), el_norm.replace('_', '.')):
                for suff in ('_electricity', '.electricity'):
                    key = _norm(base + suff)
                    if key in cols_norm:
                        chosen = cols_norm[key]; break
                if chosen: break
        if chosen is None:
            series_map[el_orig] = np.zeros(len(time_index))
        else:
            series_map[el_orig] = window_df[chosen].to_numpy()
    return series_map


def apply_recourse_for_step(residual: float, v2_row: pd.Series, pv_sched_mw: float) -> Tuple[float, float]:
    """Policy commands (open-loop) given the current residual before local actions.

    residual > 0  => deficit (need supply / discharge)
    residual < 0  => surplus (need absorption / curtail / charge)

    Returns (p_bess_cmd_mw, extra_curt_cmd_mw)

    Notes:
    - We no longer generate u_plus / u_minus here; PCC imbalance is the *post-action* residual.
    - Extra curtail only acts when residual < 0 (surplus) and is capped by scheduled PV injection (pv_sched_mw).
    - rho coefficients are not applied in OOS; imbalance settlement is the terminal residual.
    """
    d_plus = max(0.0, float(residual))
    d_minus = max(0.0, -float(residual))

    lam0 = float(v2_row.get('lambda0_mw', 0.0))
    lam_p = float(v2_row.get('lambda_plus', 0.0))
    lam_m = float(v2_row.get('lambda_minus', 0.0))
    chi0 = float(v2_row.get('chi0_mw', 0.0))
    chi_m = float(v2_row.get('chi_minus', 0.0))

    # Unclipped BESS command (+ discharge, - charge)
    p_bess_cmd = lam0 + lam_p * d_plus - lam_m * d_minus

    extra_curt = 0.0
    if d_minus > 0.0 and pv_sched_mw > 0.0:
        # Curtail scheduled PV (cannot exceed scheduled injection)
        extra_curt = max(0.0, min(chi0 + chi_m * d_minus, pv_sched_mw))
    return p_bess_cmd, extra_curt


def main() -> None:
    os.makedirs(OUTDIR, exist_ok=True)

    v2_csv = RESULTS_CSV or _infer_v2_csv(EPSILON)
    if not v2_csv or not os.path.exists(v2_csv):
        raise FileNotFoundError(
            "Could not resolve v2 results CSV. Set RESULTS_CSV or check EPSILON token matches filename."
        )

    # Load v2 and samples
    df = _load_v2_results(v2_csv)
    temp_df, pv_df, hp_df = _load_samples(SAMPLES_DIR)
    samples_meta = _load_samples_meta(SAMPLES_DIR) or {}

    # Canonical sample index
    temp_ts = pd.to_datetime(temp_df['timestamp'])
    pv_ts = pd.to_datetime(pv_df['timestamp'])
    hp_ts = pd.to_datetime(hp_df['timestamp'])
    sample_index = pd.DatetimeIndex(sorted(set(temp_ts) & set(pv_ts) & set(hp_ts)))

    # --- Runtime overrides for mean-centering flag ---
    # Priority order: command-line arg > environment variable > user config above.
    # CLI usage examples (optional):
    #   python dso_model_v3.py --mean-centered
    #   python dso_model_v3.py --schedule-residual
    import sys as _sys
    global USE_MEAN_CENTERED_POLICY  # make explicit we may override the module-level setting
    if any(arg in ('--mean-centered', '--mean_centered') for arg in _sys.argv[1:]):
        USE_MEAN_CENTERED_POLICY = True
    elif any(arg in ('--schedule-residual', '--schedule_residual', '--no-mean-centered') for arg in _sys.argv[1:]):
        USE_MEAN_CENTERED_POLICY = False
    else:
        env_flag = os.getenv('V3_USE_MEAN_CENTERED')  # '1','true','yes' => True; '0','false','no' => False
        if env_flag is not None:
            val = env_flag.strip().lower()
            if val in ('1','true','t','yes','y'):  # truthy
                USE_MEAN_CENTERED_POLICY = True
            elif val in ('0','false','f','no','n'):  # falsy
                USE_MEAN_CENTERED_POLICY = False

    print(f"[config] USE_MEAN_CENTERED_POLICY = {USE_MEAN_CENTERED_POLICY} | residual basis = {'mean_centered' if USE_MEAN_CENTERED_POLICY else 'schedule'}")

    # Fix v2 timeline if needed
    index = df.index
    index_is_datetime = isinstance(index, pd.DatetimeIndex)
    needs_timeline_fix = (not index_is_datetime) or (index_is_datetime and len(index) > 0 and getattr(index[0], 'year', 1970) < 2000)
    if needs_timeline_fix:
        if len(sample_index) >= len(df):
            index = sample_index[:len(df)]
        else:
            min_len = min(len(sample_index), len(df))
            index = sample_index[:min_len]
            df = df.iloc[:min_len].copy()
        df.index = index

    # dt_hours
    if 'meta_dt_hours' in df.columns:
        try:
            dt_hours = float(df['meta_dt_hours'].iloc[0])
        except Exception:
            dt_hours = float(samples_meta.get('dt_hours', 1.0))
    else:
        dt_hours = float(samples_meta.get('dt_hours', 0.0))
        if not np.isfinite(dt_hours) or dt_hours <= 0:
            try:
                dt_hours = float(pd.Series(index).diff().dropna().median() / pd.Timedelta(hours=1))
            except Exception:
                dt_hours = 1.0
        if not np.isfinite(dt_hours) or dt_hours <= 0:
            dt_hours = 1.0

    price = (
        df['electricity_price_eur_mwh'].to_numpy(dtype=float)
        if 'electricity_price_eur_mwh' in df.columns
        else np.zeros(len(index))
    )

    # Load HP predictor coefficients from v2 export meta (fallback to v2 defaults)
    def _get_meta_float(col: str, default: float) -> float:
        try:
            return float(df.get(col, [default])[0])
        except Exception:
            return default
    HP_PRED_PMAX = _get_meta_float('meta_hp_pred_pmax_mw', 0.30)
    HP_COEFF_B0 = _get_meta_float('meta_hp_coeff_b0', 0.331877)
    HP_COEFF_BHDD = _get_meta_float('meta_hp_coeff_bhdd', 0.015908)
    HP_COEFF_BPI = _get_meta_float('meta_hp_coeff_bpi', -0.000492)
    HP_COEFF_BTAV = _get_meta_float('meta_hp_coeff_btav', -0.014595)
    HP_COEFF_A1 = _get_meta_float('meta_hp_coeff_a1', 0.013139)
    HP_COEFF_A2 = _get_meta_float('meta_hp_coeff_a2', -0.006540)
    HP_PRED_TBASE_C = 10.0

    # Baseline map for HP predictor (per timestamp)
    try:
        base_df = pd.read_csv('hp_baseline_profile.csv')
        _base_map = {(int(r.weekday), int(r.hour), int(r.minute)): float(r.P_base_MW) for r in base_df.itertuples(index=False)}
        _base_mean = float(base_df['P_base_MW'].mean())
    except Exception:
        _base_map = {}
        _base_mean = 0.0
    def baseline_lookup(dt: pd.Timestamp) -> float:
        return _base_map.get((dt.weekday(), dt.hour, dt.minute), _base_mean)

    # Baselines
    # Day-ahead external net import (import - export) for settlement baseline
    if 'ext_grid_import_mw' in df.columns and 'ext_grid_export_mw' in df.columns:
        base_ext_net = (df['ext_grid_import_mw'].to_numpy(dtype=float) - df['ext_grid_export_mw'].to_numpy(dtype=float))
    else:
        # Fallback to net_grid_power_mw (may already embed internal balances)
        base_ext_net = df['net_grid_power_mw'].to_numpy(dtype=float) if 'net_grid_power_mw' in df.columns else np.zeros(len(index))
    base_net_import = base_ext_net  # keep legacy name for downstream references
    pv_bus_ids = _get_bus_ids_from_columns(df, 'pv_avail_bus_', '_mw')
    pv_gen_bus_ids = _get_bus_ids_from_columns(df, 'pv_gen_bus_', '_mw')
    # Initial HP bus ids inferred from v2 result columns (may include artifacts if naming pattern over-matches)
    hp_bus_ids_raw = _get_bus_ids_from_columns(df, 'hp_elec_bus_', '_mw')
    base_pv_by_bus = _collect_da_series(df, pv_bus_ids, 'pv_avail_bus_', '_mw')
    base_pv_da_by_bus = _collect_da_series(df, pv_gen_bus_ids, 'pv_gen_bus_', '_mw')
    base_hp_by_bus_raw = _collect_da_series(df, hp_bus_ids_raw, 'hp_elec_bus_', '_mw')
    # Flexible load exports (baseline, realized, curtailment) added in v2 refactor
    flex_baseline_bus_ids = _get_bus_ids_from_columns(df, 'baseline_flex_bus_', '_mw')
    flex_realized_bus_ids = _get_bus_ids_from_columns(df, 'flex_load_bus_', '_mw')
    flex_curt_bus_ids = _get_bus_ids_from_columns(df, 'curt_bus_', '_mw')
    flex_baseline_by_bus = _collect_da_series(df, flex_baseline_bus_ids, 'baseline_flex_bus_', '_mw') if flex_baseline_bus_ids else {}
    flex_realized_by_bus = _collect_da_series(df, flex_realized_bus_ids, 'flex_load_bus_', '_mw') if flex_realized_bus_ids else {}
    flex_curt_by_bus = _collect_da_series(df, flex_curt_bus_ids, 'curt_bus_', '_mw') if flex_curt_bus_ids else {}
    # Aggregate flexible load series (they are deterministic across OOS trajectories)
    if flex_baseline_by_bus:
        flex_baseline_total = np.sum(list(flex_baseline_by_bus.values()), axis=0)
    else:
        flex_baseline_total = np.zeros(len(index))
    if flex_realized_by_bus:
        flex_realized_total = np.sum(list(flex_realized_by_bus.values()), axis=0)
    else:
        flex_realized_total = np.zeros(len(index))
    if flex_curt_by_bus:
        flex_curt_total = np.sum(list(flex_curt_by_bus.values()), axis=0)
    else:
        # Fallback: difference baseline - realized if curtail columns absent
        flex_curt_total = np.maximum(0.0, flex_baseline_total - flex_realized_total)
    # ycap_mw (aggregate curtailment) consistency check
    if 'ycap_mw' in df.columns:
        try:
            ycap_series = df['ycap_mw'].to_numpy(dtype=float)
            # Only warn if discrepancy is material
            diff_norm = np.max(np.abs(ycap_series - flex_curt_total)) if len(ycap_series) else 0.0
            if diff_norm > 1e-6:
                print(f"[flex-load] Warning: ycap_mw mismatch vs summed curt_bus_ series (max abs diff {diff_norm:.3e})")
        except Exception:
            ycap_series = None
    else:
        ycap_series = None
    # Pre-compute flexible load energy / power metrics (constant across trajectories)
    if dt_hours > 0 and len(flex_baseline_total):
        flex_metrics_static = {
            'flex_baseline_energy_mwh': float(np.sum(flex_baseline_total * dt_hours)),
            'flex_realized_energy_mwh': float(np.sum(flex_realized_total * dt_hours)),
            'flex_curtail_energy_mwh': float(np.sum(flex_curt_total * dt_hours)),
            'flex_avg_curtailment_mw': float(np.mean(flex_curt_total)),
            'flex_peak_curtailment_mw': float(np.max(flex_curt_total)) if len(flex_curt_total) else 0.0,
        }
        baseline_energy = flex_metrics_static['flex_baseline_energy_mwh']
        if baseline_energy > 0:
            flex_metrics_static['flex_energy_curtailed_pct'] = 100.0 * flex_metrics_static['flex_curtail_energy_mwh'] / baseline_energy
        else:
            flex_metrics_static['flex_energy_curtailed_pct'] = np.nan
        if ycap_series is not None and len(ycap_series) == len(flex_curt_total):
            flex_metrics_static['flex_ycap_energy_mwh'] = float(np.sum(ycap_series * dt_hours))
    else:
        flex_metrics_static = {
            'flex_baseline_energy_mwh': np.nan,
            'flex_realized_energy_mwh': np.nan,
            'flex_curtail_energy_mwh': np.nan,
            'flex_avg_curtailment_mw': np.nan,
            'flex_peak_curtailment_mw': np.nan,
            'flex_energy_curtailed_pct': np.nan,
        }
    # Log detection summary
    if flex_baseline_by_bus:
        print(f"[flex-load] Detected {len(flex_baseline_by_bus)} flexible load buses (baseline). Total baseline energy = {flex_metrics_static['flex_baseline_energy_mwh']:.6f} MWh")
        print(f"[flex-load] Realized energy = {flex_metrics_static['flex_realized_energy_mwh']:.6f} MWh | Curtailment energy = {flex_metrics_static['flex_curtail_energy_mwh']:.6f} MWh ({flex_metrics_static['flex_energy_curtailed_pct']:.3f}%)")
    else:
        print("[flex-load] No flexible load columns detected in v2 results CSV.")
    # DA PV setpoints (what was actually scheduled in v2)
    base_pv_da_total = np.sum(list(base_pv_da_by_bus.values()), axis=0) if base_pv_da_by_bus else np.zeros(len(index))
    # Forecast (mean) PV availability total for mean-centering policy residuals
    pv_mean_total = np.sum(list(base_pv_by_bus.values()), axis=0) if base_pv_by_bus else np.zeros(len(index))

    # Filter samples to our timestamps
    temp_df = temp_df[temp_df['timestamp'].isin(index)]
    pv_df = pv_df[pv_df['timestamp'].isin(index)]
    hp_df = hp_df[hp_df['timestamp'].isin(index)]

    # Sample IDs
    sample_ids = sorted(set(pv_df['sample_id'].unique()) & set(hp_df['sample_id'].unique()) & set(temp_df['sample_id'].unique()))
    if MAX_TRAJ is not None:
        sample_ids = sample_ids[: int(MAX_TRAJ)]
    # Optional environment override for quick testing (does not alter stored config)
    env_max = os.getenv('V3_MAX_TRAJ')
    if env_max:
        try:
            env_lim = int(env_max)
            if env_lim > 0:
                sample_ids = sample_ids[:env_lim]
                print(f"[v3] ENV override: limiting trajectories to {env_lim} via V3_MAX_TRAJ")
        except Exception:
            pass
    if not sample_ids:
        raise RuntimeError("No overlapping sample_ids found across PV/HP/Temperature samples")

    # Build network and matrices once
    net = create_network_from_csv()
    slack_bus_index = int(net.ext_grid.bus.iloc[0])
    non_slack_buses = [b for b in net.bus.index if b != slack_bus_index]
    # Build complex Ybus including lines and transformers
    Ybus = build_Ybus(net)
    Ybus_reduced = np.delete(np.delete(Ybus, slack_bus_index, axis=0), slack_bus_index, axis=1)
    # Use pseudo-inverse if singular
    try:
        Zbus_reduced = np.linalg.inv(Ybus_reduced)
    except np.linalg.LinAlgError:
        Zbus_reduced = np.linalg.pinv(Ybus_reduced)
    Rmat = np.real(Zbus_reduced); X = np.imag(Zbus_reduced)
    # For DC angle approximation: B' = -Im(Ybus)
    Bprime = -np.imag(Ybus)
    Bprime_reduced = np.delete(np.delete(Bprime, slack_bus_index, axis=0), slack_bus_index, axis=1)
    try:
        Bprime_inv = np.linalg.inv(Bprime_reduced)
    except np.linalg.LinAlgError:
        Bprime_inv = np.linalg.pinv(Bprime_reduced)

    # Re-identify HP load buses directly from the network (names starting with 'HP'),
    # ensuring "per-device" semantics (Option A). This guards against accidental
    # inflation from column pattern matching.
    try:
        hp_load_buses_from_net = sorted(set(
            net.load.loc[
                net.load['name'].astype(str).str.startswith('HP', na=False), 'bus'
            ].astype(int).tolist()
        ))
    except Exception:
        hp_load_buses_from_net = []

    # Intersect with raw v2 column-derived list to avoid including buses that have
    # no DA schedule (conservative) but fall back to network list if intersection empty.
    hp_bus_ids_final = sorted(set(hp_load_buses_from_net) & set(base_hp_by_bus_raw.keys()))
    if not hp_bus_ids_final:
        hp_bus_ids_final = hp_load_buses_from_net if hp_load_buses_from_net else list(base_hp_by_bus_raw.keys())

    # Rebuild base_hp_by_bus using the final set
    base_hp_by_bus = {b: base_hp_by_bus_raw[b] for b in hp_bus_ids_final if b in base_hp_by_bus_raw}

    # Diagnostic: compare per-device average DA schedule vs per-device predictor capacity
    if base_hp_by_bus:
        da_hp_total_series = np.sum(list(base_hp_by_bus.values()), axis=0)
        da_hp_mean_total = float(np.mean(da_hp_total_series)) if len(da_hp_total_series) else 0.0
        n_hp_devices = len(base_hp_by_bus)
        da_hp_mean_per_device = da_hp_mean_total / max(1, n_hp_devices)
        print(f"[HP-Scaling] Detected {n_hp_devices} HP devices from network (Option A).")
        print(f"[HP-Scaling] DA total HP mean = {da_hp_mean_total:.6f} MW | per-device mean = {da_hp_mean_per_device:.6f} MW")
        print(f"[HP-Scaling] HP_PRED_PMAX (per device) = {HP_PRED_PMAX:.6f} MW")
    else:
        print("[HP-Scaling] No HP devices detected; predictor deviation will be zero.")

    # Load mapping for non-flex loads from VDI
    time_index = index
    elec_ts_map = build_electrical_time_series(time_index)

    # Helpers for sgen caps
    if 'type' in net.sgen.columns:
        pv_mask = net.sgen['type'].astype(str).str.contains('PV', na=False)
        bess_mask = net.sgen['type'].astype(str).str.contains('BESS', na=False)
    else:
        pv_mask = pd.Series([True] * len(net.sgen), index=net.sgen.index)
        bess_mask = pd.Series([False] * len(net.sgen), index=net.sgen.index)
    pv_caps_by_bus = net.sgen.loc[pv_mask].groupby('bus')['p_mw'].sum().to_dict()
    bess_caps_by_bus = net.sgen.loc[bess_mask].groupby('bus')['p_mw'].sum().to_dict()
    bess_pmax_total = float(sum(bess_caps_by_bus.values())) if bess_caps_by_bus else 0.0

    # BESS meta from v2 (capacity MWh, initial SOC fraction, efficiency)
    try:
        bess_eff = float(df.get('meta_bess_eff', [0.95])[0])
    except Exception:
        bess_eff = 0.95
    try:
        bess_initial_soc = float(df.get('meta_bess_initial_soc', [0.5])[0])
    except Exception:
        bess_initial_soc = 0.5
    # total capacity across all BESS units (MWh)
    try:
        total_bess_capacity_mwh = float(df.get('bess_total_capacity_mwh', [np.nan])[0])
        if not np.isfinite(total_bess_capacity_mwh) or total_bess_capacity_mwh <= 0:
            raise ValueError()
    except Exception:
        # fallback: per-unit capacity if present multiplied by number of BESS buses
        try:
            per_bess_cap = float(df.get('meta_bess_capacity_mwh', [np.nan])[0])
        except Exception:
            per_bess_cap = np.nan
        if not np.isfinite(per_bess_cap) or per_bess_cap <= 0:
            # final fallback guess: 0.25 MWh per BESS bus
            per_bess_cap = 0.25
        total_bess_capacity_mwh = per_bess_cap * (len(bess_caps_by_bus) if bess_caps_by_bus else 0)

    # Precompute line admittances and limits for loading eval
    sqrt3 = float(np.sqrt(3.0))
    Sbase = float(net.sn_mva)  # MVA
    line_data = []  # tuples: (from_bus, to_bus, y_pu, Imax_kA, Vbase_kV_from)
    for line in net.line.itertuples():
        fb = int(line.from_bus); tb = int(line.to_bus)
        Vbase_kV = float(net.bus.at[fb, 'vn_kv'])
        Z_base = (Vbase_kV ** 2) / Sbase
        r_pu = float(line.r_ohm_per_km) * float(line.length_km) / Z_base
        x_pu = float(line.x_ohm_per_km) * float(line.length_km) / Z_base
        z = complex(r_pu, x_pu)
        if abs(z) < 1e-12:
            continue
        y = 1.0 / z
        Imax_kA = float(getattr(line, 'max_i_ka', 0.0))
        line_data.append((fb, tb, y, Imax_kA, Vbase_kV))

    # Precompute transformer admittances and ratings for loading eval (LV side)
    trafo_data = []  # tuples: (hv_bus, lv_bus, y_tr_pu, sn_mva_tr)
    if hasattr(net, 'trafo') and len(net.trafo) > 0:
        for tr in net.trafo.itertuples():
            hv = int(tr.hv_bus); lv = int(tr.lv_bus)
            sn_tr = float(tr.sn_mva) if hasattr(tr, 'sn_mva') else Sbase
            vk_pu = float(tr.vk_percent) / 100.0 if hasattr(tr, 'vk_percent') else 0.05
            r_pu = float(tr.vkr_percent) / 100.0 if hasattr(tr, 'vkr_percent') else 0.01
            x_sq = max(vk_pu**2 - r_pu**2, 1e-8)
            x_pu = float(np.sqrt(x_sq))
            z_tr_pu_own = complex(r_pu, x_pu)
            z_tr_pu_sys = z_tr_pu_own * (Sbase / max(sn_tr, 1e-6))
            if abs(z_tr_pu_sys) < 1e-12:
                continue
            y_tr = 1.0 / z_tr_pu_sys
            trafo_data.append((hv, lv, y_tr, sn_tr))

    # Column groups in samples
    pv_bus_cols = [c for c in pv_df.columns if c.startswith('pv_bus_') and c.endswith('_mw')]
    hp_resid_cols = [c for c in hp_df.columns if c.startswith('hp_residual_bus_') and c.endswith('_mw')]

    per_traj_summary: List[Dict[str, float]] = []

    # Settlement & RT cost factors (DA + imbalance model)
    IMB_UP_FACTOR = 1.3  # premium multiplier for upward imbalance energy (deficit)
    IMB_DN_FACTOR = 1.3  # premium multiplier for downward imbalance energy (surplus absorption)
    PV_CURT_PRICE_FACTOR = 1.0  # opportunity cost factor for curtailed scheduled PV (if any)
    BESS_THROUGHPUT_COST_EUR_PER_MWH = 0.5  # optional degradation proxy

    # Day-ahead energy cost (constant across trajectories): only pay for DA scheduled imports
    da_import_mw = np.maximum(base_net_import, 0.0)
    try:
        da_energy_cost_eur_const = float(np.sum(da_import_mw * price * dt_hours))
    except Exception:
        da_energy_cost_eur_const = 0.0

    # Precompute constant Q factors
    qfactor_household_const = float(np.tan(np.arccos(0.98)))
    qfactor_hp_const = float(np.tan(np.arccos(0.99)))

    # Prepare transformer loading logging
    trafo_loading_buffer: List[tuple] = []  # (sample_id, t_idx, trafo_index, loading_pct_float32)
    trafo_meta_written = False
    trafo_meta_records: List[Dict[str, object]] = []
    trafo_count = len(trafo_data)
    if LOG_TRAFO_LOADING and trafo_count > 0:
        os.makedirs(os.path.join(OUTDIR, TRAFO_LOADING_DIR_NAME), exist_ok=True)
        # Build meta once from trafo_data order
        for idx_tr, (hv, lv, y_tr, sn_tr) in enumerate(trafo_data):
            trafo_meta_records.append({
                'trafo_index': idx_tr,
                'hv_bus': int(hv),
                'lv_bus': int(lv),
                'sn_mva': float(sn_tr)
            })

    # Collect SoC trajectories for envelope export
    collect_soc_frac: List[np.ndarray] = []

    total = len(sample_ids)
    report_step = max(1, total // 20)  # ~5% steps
    for i, sid in enumerate(sample_ids, start=1):
        if i % report_step == 0 or i == 1 or i == total:
            print(f"[v3] processing sample {i}/{total} ({int(100*i/total)}%)...", flush=True)
        # Slice samples
        temp_sid = temp_df[temp_df['sample_id'] == sid].sort_values('timestamp')
        pv_sid = pv_df[pv_df['sample_id'] == sid].sort_values('timestamp')
        hp_sid = hp_df[hp_df['sample_id'] == sid].sort_values('timestamp')

        # Aggregate sampled PV availability and HP residuals
        pv_avail_rt = pv_sid[pv_bus_cols].to_numpy(dtype=float) if pv_bus_cols else np.zeros((len(index), 0))
        hp_resid_rt = hp_sid[hp_resid_cols].to_numpy(dtype=float) if hp_resid_cols else np.zeros((len(index), 0))
        pv_total_rt = pv_avail_rt.sum(axis=1) if pv_avail_rt.size else np.zeros(len(index))
        hp_resid_total = hp_resid_rt.sum(axis=1) if hp_resid_rt.size else np.zeros(len(index))
        if IGNORE_HP_RESIDUAL:
            hp_resid_total[:] = 0.0  # discard residual noise component

        # Build DA per-step data for recourse
        # Compute temperature-driven HP prediction for this sample using v2 predictor
        # Daily mean temperature per date (C) for the sample
        temp_sid = temp_sid.copy()
        temp_sid['date'] = pd.to_datetime(temp_sid['timestamp']).dt.date
        daily_mean_temp = temp_sid.groupby('date')['temperature_c'].mean().to_dict()
        # Predicted per-bus HP (same value per HP bus as in v2), then aggregate across HP buses
        # Option A: per-device prediction scaled by number of active HP devices (hp_bus_ids_final)
        n_hp = max(1, len(hp_bus_ids_final))
        hp_pred_total = np.zeros(len(index))
        for t_idx, ts in enumerate(index):
            T_C_t = float(temp_sid['temperature_c'].iloc[t_idx]) if t_idx < len(temp_sid) else float('nan')
            if not np.isfinite(T_C_t):
                T_C_t = 20.0
            price_t = float(price[t_idx]) if t_idx < len(price) else 0.0
            HDD_t = max(0.0, HP_PRED_TBASE_C - T_C_t)
            T_avg_d = float(daily_mean_temp.get(ts.date(), T_C_t))
            tod = ts.hour + ts.minute / 60.0
            sin24 = np.sin(2.0 * np.pi * tod / 24.0)
            cos24 = np.cos(2.0 * np.pi * tod / 24.0)
            y_dev = (
                HP_COEFF_B0
                + HP_COEFF_BHDD * HDD_t
                + HP_COEFF_BPI * price_t
                + HP_COEFF_BTAV * T_avg_d
                + HP_COEFF_A1 * sin24
                + HP_COEFF_A2 * cos24
            )
            P_t = baseline_lookup(ts) + HP_PRED_PMAX * y_dev
            P_t = min(max(P_t, 0.0), HP_PRED_PMAX)
            hp_pred_total[t_idx] = P_t * n_hp

        # Sanity check: if predicted total is implausibly high vs DA schedule, warn
        if base_hp_by_bus:
            da_hp_total = np.sum(list(base_hp_by_bus.values()), axis=0)
            da_mean = float(np.mean(da_hp_total)) if len(da_hp_total) else 0.0
            pred_mean = float(np.mean(hp_pred_total)) if len(hp_pred_total) else 0.0
            if da_mean > 0 and pred_mean > 5 * da_mean:
                print(f"⚠️  [HP-Scaling Warning] Predictor mean {pred_mean:.4f} MW is >5× DA mean {da_mean:.4f} MW. Check coefficients or baseline.")

        # DA total HP from v2 (aggregate over finalized HP device buses)
        da_hp_total = np.sum([base_hp_by_bus.get(b, np.zeros(len(index))) for b in base_hp_by_bus.keys()], axis=0) if base_hp_by_bus else np.zeros(len(index))
        hp_temp_dev_total = hp_pred_total - da_hp_total

        # Components of deviation per trajectory (arrays length = horizon)
        hp_dev = hp_temp_dev_total + hp_resid_total
        pv_dev = base_pv_da_total - pv_total_rt  # signed PV deviation vs DA schedule (for reporting)
        base_residual = hp_dev + pv_dev          # schedule-based residual
        # Mean-centered policy residual if enabled:
        # hp mean deviation = -hp_resid_total (hp_pred_total used as mean predictor)
        # pv mean deviation = pv_mean_total - pv_total_rt
        # residual_policy_array = hp_mean_dev + pv_mean_dev
        if USE_MEAN_CENTERED_POLICY:
            pv_mean_dev = pv_mean_total - pv_total_rt
            residual_policy_array = (-hp_resid_total) + pv_mean_dev
        else:
            residual_policy_array = base_residual  # legacy schedule-based activation

        # Accumulators for metrics and voltages
        net_import_rt = np.zeros(len(index))
        v_min = 10.0; v_max = 0.0
        max_line_loading = 0.0; max_trafo_loading = 0.0
        steps_line_over_thresh = 0; steps_trafo_over_thresh = 0; steps_voltage_violation = 0
        pv_curtail_energy = 0.0; bess_rt_energy_throughput = 0.0
        pv_rt_curt_cost_eur = 0.0; imbalance_cost_eur = 0.0; bess_cycle_cost_eur = 0.0

        # Initialize BESS state for this trajectory
        E_mwh = max(0.0, min(1.0, bess_initial_soc)) * max(0.0, total_bess_capacity_mwh)
        bess_soc_clip_steps = 0; u_plus_energy = 0.0; u_minus_energy = 0.0
        # Track SoC fraction series for this trajectory
        soc_series = np.zeros(len(index), dtype=float)

        # Collect policy residual stats if enabled
        residual_policy_vals = [] if USE_MEAN_CENTERED_POLICY else None

        for t, ts in enumerate(index):
            row = df.iloc[t]
            pv_sched_t = float(base_pv_da_total[t])
            resid_t = float(base_residual[t])  # schedule-referenced residual used for settlement evolution

            # Choose residual signal for policy activation
            if USE_MEAN_CENTERED_POLICY:
                resid_policy_t = float(residual_policy_array[t])
                residual_policy_vals.append(resid_policy_t)
            else:
                resid_policy_t = resid_t

            # Policy open-loop commands from affine recourse (based on chosen residual)
            p_bess_cmd, extra_curt_cmd = apply_recourse_for_step(resid_policy_t, row, pv_sched_t)

            # --- Battery-first: project affine command onto feasible (power + energy) band ---
            p_cmd = float(p_bess_cmd)
            # Clip to nameplate power limits
            if bess_pmax_total > 0:
                p_cmd = max(-bess_pmax_total, min(bess_pmax_total, p_cmd))
            else:
                p_cmd = 0.0

            if dt_hours > 0 and bess_eff > 0 and total_bess_capacity_mwh > 0:
                # Energy-based instantaneous feasible range (positive discharge, negative charge)
                p_max_energy = (E_mwh * bess_eff) / dt_hours  # max discharge allowed
                charge_headroom = max(0.0, total_bess_capacity_mwh - E_mwh)
                p_min_energy = - (charge_headroom / (bess_eff * dt_hours))  # max (negative) charging
                # Combine with power bounds
                p_min_total = max(-bess_pmax_total, p_min_energy)
                p_max_total = min(bess_pmax_total, p_max_energy)
                if p_min_total > p_max_total:
                    # Degenerate band (numerical), collapse to 0
                    p_min_total, p_max_total = 0.0, 0.0
                p_real = min(max(p_cmd, p_min_total), p_max_total)
            else:
                # No meaningful capacity/efficiency info -> already power clipped
                p_real = p_cmd if bess_pmax_total > 0 else 0.0

            # SoC update (charge if p_real < 0, discharge if p_real > 0)
            if dt_hours > 0 and bess_eff > 0 and total_bess_capacity_mwh > 0:
                E_mwh = E_mwh + max(0.0, -p_real) * bess_eff * dt_hours - max(0.0, p_real) / bess_eff * dt_hours
                E_mwh = max(0.0, min(total_bess_capacity_mwh, E_mwh))
            else:
                # Track clip if non-zero command requested but no capacity
                if abs(p_real) > 1e-9:
                    bess_soc_clip_steps += 1
            # Record SoC fraction after update
            soc_series[t] = (E_mwh / total_bess_capacity_mwh) if total_bess_capacity_mwh > 0 else 0.0

            bess_delta = p_real  # (+) discharge reduces deficit/import
            resid_t -= bess_delta
            if abs(p_real - p_bess_cmd) > 1e-9:
                bess_soc_clip_steps += 1

            # --- PV curtailment after battery action (only for remaining surplus) ---
            extra_curt_used = 0.0
            if resid_t < 0 and extra_curt_cmd > 0:
                pv_avail_t = float(pv_total_rt[t])  # real-time available PV (aggregate)
                extra_curt_used = min(extra_curt_cmd, pv_avail_t)
                resid_t += extra_curt_used  # curtail reduces injection (moves residual toward zero)

            # Final PCC imbalance after local actions (signed residual -> two-sided imbalance)
            u_plus = max(0.0, resid_t)
            u_minus = max(0.0, -resid_t)

            # Realized net import (DA base + imbalance settlement)
            net_import_rt[t] = base_net_import[t] + u_plus - u_minus

            # Metrics accumulation
            pv_curtail_energy += float(extra_curt_used) * dt_hours
            bess_rt_energy_throughput += float(abs(bess_delta)) * dt_hours
            u_plus_energy += float(u_plus) * dt_hours
            u_minus_energy += float(u_minus) * dt_hours

            # Costs (DA energy already accounted once; here only RT recourse premiums/opportunity costs)
            p_price = float(price[t]) if t < len(price) else 0.0
            pv_rt_curt_cost_eur += float(extra_curt_used) * PV_CURT_PRICE_FACTOR * p_price * dt_hours
            imbalance_cost_eur += (IMB_UP_FACTOR * float(u_plus) + IMB_DN_FACTOR * float(u_minus)) * p_price * dt_hours
            bess_cycle_cost_eur += float(abs(bess_delta)) * float(BESS_THROUGHPUT_COST_EUR_PER_MWH) * dt_hours

            # Build P/Q injections per bus for LDF
            P_inj = {b: 0.0 for b in net.bus.index}
            Q_inj = {b: 0.0 for b in net.bus.index}

            # Non-flex loads
            for load in net.load.itertuples():
                b = load.bus; name = getattr(load, 'name', '')
                p_kw = float(elec_ts_map.get(name, np.zeros(len(index)))[t]) if name in elec_ts_map else 0.0
                p_mw = p_kw / 1000.0
                P_inj[b] -= p_mw
                # approximate Q using household pf (match v2)
                Q_inj[b] -= p_mw * qfactor_household_const

            # Flexible loads (day-ahead realized schedule after curtailment is fixed across OOS)
            # We subtract flex_realized (already MW). Treat pf same as household (could customize later).
            if flex_realized_by_bus:
                for b, series in flex_realized_by_bus.items():
                    if t < len(series):
                        p_flex = float(series[t])
                        if abs(p_flex) > 0.0:
                            P_inj[b] -= p_flex
                            Q_inj[b] -= p_flex * qfactor_household_const

            # Heat pumps: use DA hp_elec_bus_*_mw plus residual split proportionally across HP buses
            if base_hp_by_bus:
                # Distribute total HP deviation equally across detected HP devices (Option A)
                total_dev_t = float(hp_temp_dev_total[t] + hp_resid_total[t])
                per_bus_dev = total_dev_t / max(1, len(base_hp_by_bus))
                for b in base_hp_by_bus.keys():
                    p_da = float(base_hp_by_bus.get(b, np.zeros(len(index)))[t])
                    adj_p = p_da + per_bus_dev
                    P_inj[b] -= adj_p
                    Q_inj[b] -= adj_p * qfactor_hp_const

            # PV injections: realized PV after extra curtail; distribute proportionally to installed caps
            if pv_bus_ids:
                total_cap = sum(pv_caps_by_bus.get(b, 0.0) for b in pv_bus_ids)
                pv_effective = max(0.0, pv_total_rt[t] - extra_curt_used)
                for b in pv_bus_ids:
                    share = (pv_caps_by_bus.get(b, 0.0) / total_cap) if total_cap > 0 else 0.0
                    P_inj[b] += pv_effective * share

            # BESS: implement delta at dominant BESS bus (or distribute)
            if bess_pmax_total > 0 and bess_caps_by_bus:
                # allocate by capacity share
                for b, cap in bess_caps_by_bus.items():
                    share = cap / bess_pmax_total if bess_pmax_total > 0 else 0.0
                    P_inj[b] += bess_delta * share

            # Slack net import consistency (not strictly enforced here; ext_grid covers mismatch)
            # Convert to per-unit and solve V_reduced
            P_pu = np.array([P_inj[b] / net.sn_mva for b in non_slack_buses])
            Q_pu = np.array([Q_inj[b] / net.sn_mva for b in non_slack_buses])
            V_red = 1.0 + 2.0 * (Rmat @ P_pu + X @ Q_pu)

            # Track voltage stats
            v_min = min(v_min, float(np.min(V_red)))
            v_max = max(v_max, float(np.max(V_red)))
            # Count voltage violations against typical 0.95-1.05 limits
            if np.any(V_red < 0.95) or np.any(V_red > 1.05):
                steps_voltage_violation += 1

            # Build full-bus voltage vector (mag + angle) for branch loading eval
            # DC angle approx
            theta_red = Bprime_inv @ P_pu if len(P_pu) > 0 else np.zeros(0)
            theta = np.zeros(len(net.bus))
            # Insert non-slack angles in order of non_slack_buses
            for k, b in enumerate(non_slack_buses):
                theta[b] = theta_red[k] if k < len(theta_red) else 0.0
            # Voltage magnitudes
            Vmag = np.ones(len(net.bus))
            for k, b in enumerate(non_slack_buses):
                Vmag[b] = V_red[k]
            # Complex voltages
            Vcomplex = Vmag * np.exp(1j * theta)

            # Line loading
            any_line_over_thresh = False
            for fb, tb, y, Imax_kA, Vbase_kV in line_data:
                if Imax_kA <= 0:
                    continue
                I_pu = (Vcomplex[fb] - Vcomplex[tb]) * y
                I_kA = abs(I_pu) * (Sbase / (sqrt3 * Vbase_kV))
                loading_pct = (I_kA / Imax_kA) * 100.0
                if loading_pct > max_line_loading:
                    max_line_loading = float(loading_pct)
                if loading_pct > LOADING_VIOLATION_THRESHOLD_PCT:
                    any_line_over_thresh = True
            if any_line_over_thresh:
                steps_line_over_thresh += 1

            # Transformer loading (LV side apparent power vs nameplate)
            any_trafo_over_thresh = False
            for idx_tr, (hv, lv, y_tr, sn_tr) in enumerate(trafo_data):
                I_lv_pu = (Vcomplex[lv] - Vcomplex[hv]) * y_tr
                S_lv_pu = Vcomplex[lv] * np.conj(I_lv_pu)
                S_lv_MVA = abs(S_lv_pu) * Sbase
                if sn_tr > 0:
                    loading_pct = (S_lv_MVA / sn_tr) * 100.0
                    if loading_pct > max_trafo_loading:
                        max_trafo_loading = float(loading_pct)
                    if loading_pct > LOADING_VIOLATION_THRESHOLD_PCT:
                        any_trafo_over_thresh = True
                    if LOG_TRAFO_LOADING:
                        trafo_loading_buffer.append((sid, t, idx_tr, np.float32(loading_pct)))
            if any_trafo_over_thresh:
                steps_trafo_over_thresh += 1

        # Flush buffer periodically
        if LOG_TRAFO_LOADING and trafo_loading_buffer and (i % TRAFO_LOADING_BUFFER_SAMPLES == 0 or i == total):
            try:
                import pandas as _pd
                buf_df = _pd.DataFrame(trafo_loading_buffer, columns=['sample_id', 't', 'trafo_index', 'loading_pct'])
                # Write/append parquet
                token = _epsilon_token(EPSILON)
                out_dir = os.path.join(OUTDIR, TRAFO_LOADING_DIR_NAME)
                out_path = os.path.join(out_dir, f"{TRAFO_LOADING_FILENAME_PREFIX}{token}.parquet")
                if TRAFO_LOADING_WRITE_PARQUET:
                    # Append mode: if file exists, concat then overwrite (simpler than row-group append without pyarrow writer state)
                    if os.path.exists(out_path):
                        existing = _pd.read_parquet(out_path)
                        buf_df = _pd.concat([existing, buf_df], ignore_index=True)
                    buf_df['loading_pct'] = buf_df['loading_pct'].astype(TRAFO_LOADING_FLOAT_DTYPE)
                    buf_df.to_parquet(out_path, index=False)
                else:
                    # Fallback CSV
                    if os.path.exists(out_path.replace('.parquet', '.csv')):
                        buf_df.to_csv(out_path.replace('.parquet', '.csv'), mode='a', header=False, index=False)
                    else:
                        buf_df.to_csv(out_path.replace('.parquet', '.csv'), index=False)
                trafo_loading_buffer.clear()
            except Exception as e:
                print(f"[WARN] Failed to write transformer loading buffer: {e}")

        # Summarize metrics including voltage and thermal bounds
        summ = _summarize_trajectory(index, price, net_import_rt, dt_hours, pv_total_rt, hp_resid_total, temp_sid['temperature_c'].to_numpy(dtype=float))
        summ['v_min_pu'] = v_min
        summ['v_max_pu'] = v_max
        summ['max_line_loading_pct'] = max_line_loading
        summ['max_trafo_loading_pct'] = max_trafo_loading
        summ['steps_line_over_80pct'] = int(steps_line_over_thresh)
        summ['steps_trafo_over_80pct'] = int(steps_trafo_over_thresh)
        summ['loading_violation_threshold_pct'] = float(LOADING_VIOLATION_THRESHOLD_PCT)
        summ['steps_voltage_violation'] = int(steps_voltage_violation)
        summ['pv_curtail_mwh'] = pv_curtail_energy
        summ['bess_rt_energy_throughput_mwh'] = bess_rt_energy_throughput
        summ['imbalance_mwh'] = float(u_plus_energy + u_minus_energy)
        summ['bess_soc_clip_steps'] = int(bess_soc_clip_steps)
        if USE_MEAN_CENTERED_POLICY and residual_policy_vals:
            rp_arr = np.array(residual_policy_vals, dtype=float)
            summ['residual_policy_std'] = float(np.std(rp_arr))
            summ['residual_policy_frac_negative'] = float(np.mean(rp_arr < 0))
        else:
            summ['residual_policy_std'] = np.nan
            summ['residual_policy_frac_negative'] = np.nan
        # Cost components (DA energy cost constant; store for clarity)
        summ['da_energy_cost_eur'] = float(da_energy_cost_eur_const)
        summ['rt_pv_curtail_cost_eur'] = float(pv_rt_curt_cost_eur)
        summ['rt_imbalance_cost_eur'] = float(imbalance_cost_eur)
        summ['rt_bess_cycle_cost_eur'] = float(bess_cycle_cost_eur)
        summ['total_cost_eur'] = float(da_energy_cost_eur_const + pv_rt_curt_cost_eur + imbalance_cost_eur + bess_cycle_cost_eur)
        summ['sample_id'] = sid
        # Store trajectory length and append (inside loop; previously mis-indented causing only last trajectory retained)
        summ['n_steps'] = int(len(index))
        # Attach static flexible load metrics (same for all trajectories)
        summ.update(flex_metrics_static)
        per_traj_summary.append(summ)
        collect_soc_frac.append(soc_series)

    token = _epsilon_token(EPSILON)
    # Derive DRCC mode label from v2 filename if present
    try:
        base_v2_name = os.path.basename(v2_csv) if v2_csv else ''
    except Exception:
        base_v2_name = ''
    if 'drcc_true' in base_v2_name:
        drcc_mode = 'drcc_true'
    elif 'drcc_false' in base_v2_name:
        drcc_mode = 'drcc_false'
    else:
        # Fallback: infer from meta flags we wrote (network tightening logging flag not available here, so leave generic)
        drcc_mode = 'drcc_unspecified'

    summary_df = pd.DataFrame(per_traj_summary)
    # Filename logic: omit epsilon token for deterministic (drcc_false) case
    if drcc_mode == 'drcc_false':
        summary_filename = 'v3_summary_drcc_false.csv'
    else:
        summary_filename = f'v3_summary_{drcc_mode}_epsilon_{token}.csv'
    summary_path = os.path.join(OUTDIR, summary_filename)
    summary_df.to_csv(summary_path, index=False)

    # Export SoC envelope (05/50/95) across trajectories with naming consistent to summary/meta
    soc_env_path = None
    if collect_soc_frac:
        try:
            soc_mat = np.vstack(collect_soc_frac)
            soc_p05 = np.percentile(soc_mat, 5, axis=0)
            soc_p50 = np.percentile(soc_mat, 50, axis=0)
            soc_p95 = np.percentile(soc_mat, 95, axis=0)
            soc_env_df = pd.DataFrame({
                'timestamp': index,
                'soc_p05': soc_p05,
                'soc_p50': soc_p50,
                'soc_p95': soc_p95,
            })
            if drcc_mode == 'drcc_false':
                soc_env_filename = 'soc_envelope_drcc_false.csv'
            else:
                soc_env_filename = f"soc_envelope_{drcc_mode}_epsilon_{token}.csv"
            soc_env_path = os.path.join(OUTDIR, soc_env_filename)
            soc_env_df.to_csv(soc_env_path, index=False)
            print(f"✓ SoC envelope written: {soc_env_path}")
        except Exception as e:
            print(f"[WARN] Failed to write SoC envelope: {e}")

    meta = {
        'v2_results_csv': os.path.abspath(v2_csv),
        'epsilon': EPSILON,
        'drcc_mode': drcc_mode,
        'samples_dir': os.path.abspath(SAMPLES_DIR),
        'dt_hours': float(dt_hours),
        'n_trajectories': int(len(summary_df)),
        'metrics': list(summary_df.columns),
        'write_diagnostics': WRITE_DIAGNOSTICS,
        'rt_cost_factors': {
            'imb_up_factor': IMB_UP_FACTOR,
            'imb_dn_factor': IMB_DN_FACTOR,
            'pv_curt_price_factor': PV_CURT_PRICE_FACTOR,
            'bess_throughput_cost_eur_per_mwh': BESS_THROUGHPUT_COST_EUR_PER_MWH
        },
        'ignore_hp_residual': bool(IGNORE_HP_RESIDUAL),
        'use_mean_centered_policy': bool(USE_MEAN_CENTERED_POLICY),
        'trafo_loading_logging': bool(LOG_TRAFO_LOADING)
    }
    # Add residual basis descriptor
    meta['rt_residual_basis'] = 'mean_centered' if USE_MEAN_CENTERED_POLICY else 'schedule'
    if soc_env_path:
        meta['soc_envelope_file'] = os.path.basename(soc_env_path)
    # Policy coefficients export (raw)
    policy_coeffs_path = None
    policy_cols = [c for c in [
        'lambda0_mw','lambda_plus','lambda_minus',
        'chi0_mw','chi_minus',
        'gamma0_mw','gamma_plus',
        'rho_plus_0','rho_plus_1','rho_minus_0','rho_minus_1'
    ] if c in df.columns]
    if policy_cols:
        try:
            policy_df = df[policy_cols].copy()
            policy_df.insert(0, 'timestamp', df.index)
            if drcc_mode == 'drcc_false':
                policy_coeffs_filename = 'policy_coeffs_drcc_false.csv'
            else:
                policy_coeffs_filename = f"policy_coeffs_{drcc_mode}_epsilon_{token}.csv"
            policy_coeffs_path = os.path.join(OUTDIR, policy_coeffs_filename)
            policy_df.to_csv(policy_coeffs_path, index=False)
            print(f"✓ Policy coefficients written: {policy_coeffs_path}")
            meta['policy_coeffs_file'] = os.path.basename(policy_coeffs_path)
        except Exception as e:
            print(f"[WARN] Failed to export policy coefficients: {e}")
    if LOG_TRAFO_LOADING and trafo_meta_records:
        meta['trafo_loading_file'] = os.path.join(TRAFO_LOADING_DIR_NAME, f"{TRAFO_LOADING_FILENAME_PREFIX}{_epsilon_token(EPSILON)}.parquet")
        meta['n_trafos'] = len(trafo_meta_records)
        # Write meta for transformers if not exists
        try:
            meta_path_tr = os.path.join(OUTDIR, TRAFO_LOADING_DIR_NAME, f"trafo_meta_{_epsilon_token(EPSILON)}.csv")
            if not os.path.exists(meta_path_tr):
                pd.DataFrame(trafo_meta_records).to_csv(meta_path_tr, index=False)
        except Exception as e:
            print(f"[WARN] Could not write trafo meta CSV: {e}")
    # Meta filename mirrors summary logic
    if drcc_mode == 'drcc_false':
        meta_filename = 'v3_meta_drcc_false.json'
    else:
        meta_filename = f'v3_meta_{drcc_mode}_epsilon_{token}.json'
    with open(os.path.join(OUTDIR, meta_filename), 'w', encoding='utf-8') as f:
        json.dump(meta, f, indent=2)

    print(f"✓ v3 summary written: {summary_path}")
    print(f"✓ v3 meta written: {os.path.join(OUTDIR, meta_filename)}")


if __name__ == "__main__":
    main()